name: AI Test Builder

on:
  push:
    branches:
      - "ai/test/**"
  workflow_dispatch:
    inputs:
      target_component:
        description: "Target component to test (frontend/backend/both)"
        required: false
        type: choice
        options:
          - both
          - frontend
          - backend
        default: both
      coverage_threshold:
        description: "Minimum coverage threshold (%)"
        required: false
        type: number
        default: 80
      add_missing_tests:
        description: "Add missing tests based on coverage"
        required: false
        type: boolean
        default: true
permissions:
  contents: write
  pull-requests: write
  issues: write

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  ai-test-builder:
    name: AI Test Builder & Fixer
    runs-on: ubuntu-latest
    
    outputs:
      tests_added: ${{ steps.summary.outputs.tests_added }}
      coverage_improved: ${{ steps.summary.outputs.coverage_improved }}
      errors_fixed: ${{ steps.summary.outputs.errors_fixed }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for coverage comparison

    - name: Setup Bun
      uses: ./.github/actions/setup-bun-version

    - name: Setup Python with UV for AI Scripts
      uses: ./.github/actions/setup-python-ai
      with:
        python-version: '3.11'
        cache-dependency-path: 'scripts/requirements.txt'

    # Install Dependencies
    - name: Install Root Dependencies
      run: |
        echo "📦 Installing root dependencies..."
        bun install

    - name: Install Backend Dependencies
      run: |
        echo "📦 Installing backend dependencies..."
        cd backend
        bun install

    - name: Install Frontend Dependencies
      run: |
        echo "📦 Installing frontend dependencies..."
        cd ui
        bun install

    # === FRONTEND TESTING & COVERAGE ===
    - name: Run Frontend Tests with Coverage
      if: ${{ inputs.target_component == 'frontend' || inputs.target_component == 'both' || inputs.target_component == null }}
      id: frontend-test-coverage
      continue-on-error: true
      run: |
        echo "🧪 Running frontend tests with coverage..."
        cd ui
        
        # Run tests with coverage
        bun run test:coverage --reporter=verbose --reporter=json --outputFile=test-results.json 2>&1 | tee test-output.log
        
        # Capture exit code
        echo "frontend_test_exit_code=$?" >> $GITHUB_OUTPUT
        
        # Generate coverage report
        if [ -f coverage/coverage-summary.json ]; then
          echo "📊 Coverage report generated"
          cat coverage/coverage-summary.json
        else
          echo "⚠️ No coverage report found"
        fi

    - name: Analyze Frontend Test Results
      if: ${{ (inputs.target_component == 'frontend' || inputs.target_component == 'both' || inputs.target_component == null) && steps.frontend-test-coverage.outcome != 'skipped' }}
      id: analyze-frontend
      run: |
        echo "🔍 Analyzing frontend test results..."
        cd ui
        
        # Check if tests failed (exit code OR output contains failures)
        test_exit_code="${{ steps.frontend-test-coverage.outputs.frontend_test_exit_code }}"
        has_test_failures="false"
        
        # Check exit code
        if [ "$test_exit_code" != "0" ]; then
          echo "❌ Frontend tests failed with exit code: $test_exit_code"
          has_test_failures="true"
        fi
        
        # Also check test output for failure indicators
        if [ -f test-output.log ]; then
          # Look for actual test failures, not normal test assertion output
          # Count failed tests in the summary line
          if grep -qE "([0-9]+ fail|AssertionError|TypeError:|ReferenceError:|SyntaxError:|Test failed)" test-output.log; then
            echo "❌ Frontend test output contains failures/errors"
            has_test_failures="true"
          fi
        fi
        
        if [ "$has_test_failures" = "true" ]; then
          echo "needs_fixes=true" >> $GITHUB_OUTPUT
          
          # Save error log for AI analysis
          cp test-output.log ../scripts/frontend-test-errors.log
        else
          echo "✅ Frontend tests passed"
          echo "needs_fixes=false" >> $GITHUB_OUTPUT
        fi
        
        # Analyze coverage
        if [ -f coverage/coverage-summary.json ]; then
          # Extract overall coverage percentage
          if jq . coverage/coverage-summary.json >/dev/null 2>&1; then
            coverage_pct=$(jq -r '.total.lines.pct // 0' coverage/coverage-summary.json 2>/dev/null || echo "0")
          else
            coverage_pct="0"
            echo "⚠️ Warning: coverage-summary.json is invalid"
          fi
          echo "coverage_percentage=$coverage_pct" >> $GITHUB_OUTPUT
          echo "📊 Frontend coverage: $coverage_pct%"
          
          # Check if below threshold
          threshold="${{ inputs.coverage_threshold || 80 }}"
          echo "🎯 Coverage threshold: $threshold%"
          
          # Use awk for comparison instead of bc
          if awk "BEGIN {exit !($coverage_pct < $threshold)}"; then
            echo "📉 Coverage below threshold ($threshold%), will add tests"
            echo "needs_more_tests=true" >> $GITHUB_OUTPUT
            
            # Generate uncovered files report
            find coverage -name "*.json" -not -name "coverage-summary.json" | head -20 > coverage-files.txt
          else
            echo "✅ Coverage meets threshold"
            echo "needs_more_tests=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "⚠️ No coverage report found, will add tests"
          echo "needs_more_tests=true" >> $GITHUB_OUTPUT
        fi

    # === BACKEND TESTING & COVERAGE ===
    - name: Run Backend Tests with Coverage
      if: ${{ inputs.target_component == 'backend' || inputs.target_component == 'both' || inputs.target_component == null }}
      id: backend-test-coverage
      continue-on-error: true
      run: |
        echo "🧪 Running backend tests with coverage..."
        cd backend
        
        # Check if backend has tests
        if [ ! -d "test" ] || [ ! -f "package.json" ] || ! grep -q '"test"' package.json; then
          echo "⚠️ No backend tests found, will create test setup"
          echo "backend_test_exit_code=1" >> $GITHUB_OUTPUT
          echo "No backend tests configured" > test-output.log
        else
          # Run tests with coverage
          bun run test --coverage 2>&1 | tee test-output.log
          echo "backend_test_exit_code=$?" >> $GITHUB_OUTPUT
        fi

    - name: Analyze Backend Test Results
      if: ${{ (inputs.target_component == 'backend' || inputs.target_component == 'both' || inputs.target_component == null) && steps.backend-test-coverage.outcome != 'skipped' }}
      id: analyze-backend
      run: |
        echo "🔍 Analyzing backend test results..."
        cd backend
        
        # Check if tests failed or missing (exit code OR output contains failures)
        test_exit_code="${{ steps.backend-test-coverage.outputs.backend_test_exit_code }}"
        has_test_failures="false"
        
        # Check exit code
        if [ "$test_exit_code" != "0" ]; then
          echo "❌ Backend tests failed with exit code: $test_exit_code"
          has_test_failures="true"
        fi
        
        # Also check test output for failure indicators
        if [ -f test-output.log ]; then
          # Look for actual test failures, not normal test assertion output
          # Only match actual failures (numbers > 0)
          if grep -qE "([1-9][0-9]* fail|No backend tests|AssertionError|TypeError:|ReferenceError:|SyntaxError:)" test-output.log; then
            echo "❌ Backend test output contains failures/errors or missing tests"
            has_test_failures="true"
          fi
        fi
        
        if [ "$has_test_failures" = "true" ]; then
          echo "needs_fixes=true" >> $GITHUB_OUTPUT
          
          # Save error log for AI analysis
          cp test-output.log ../scripts/backend-test-errors.log
        else
          echo "✅ Backend tests passed"
          echo "needs_fixes=false" >> $GITHUB_OUTPUT
        fi
        
        # Analyze backend coverage (parse from Bun test output)
        coverage_pct="0"
        if [ -f test-output.log ]; then
          # Extract coverage percentage from Bun test output
          # Look for line like "All files | 52.38 |"
          if grep -q "All files" test-output.log; then
            coverage_line=$(grep "All files" test-output.log | head -1)
            # Extract percentage using awk - it's the 3rd field in the table
            coverage_pct=$(echo "$coverage_line" | awk '{print $4}' | sed 's/|//g' | sed 's/%//g' | sed 's/[^0-9.]//g')
            if [ -z "$coverage_pct" ] || [ "$coverage_pct" = "0" ]; then
              # Try alternative parsing method
              coverage_pct=$(echo "$coverage_line" | grep -oE '[0-9]+\.[0-9]+' | head -1)
            fi
          fi
        fi
        
        # Fallback to 0 if parsing failed
        if [ -z "$coverage_pct" ] || ! [[ "$coverage_pct" =~ ^[0-9]+\.?[0-9]*$ ]]; then
          coverage_pct="0"
        fi
        
        echo "coverage_percentage=$coverage_pct" >> $GITHUB_OUTPUT
        echo "📊 Backend coverage: $coverage_pct%"
        
        # Check if below threshold
        threshold="${{ inputs.coverage_threshold || 80 }}"
        echo "🎯 Backend coverage threshold: $threshold%"
        
        # Use awk for comparison instead of bc
        if awk "BEGIN {exit !($coverage_pct < $threshold)}"; then
          echo "📉 Backend coverage below threshold ($threshold%), will add tests"
          echo "needs_more_tests=true" >> $GITHUB_OUTPUT
          
          # Generate uncovered files report for backend (if lcov.info exists)
          if [ -f coverage/lcov.info ]; then
            echo "📄 Found backend coverage report (lcov.info)"
            # Extract source files from lcov for context
            grep -E "^SF:" coverage/lcov.info | head -20 | sed 's/^SF://' > backend-coverage-files.txt
          else
            echo "📄 No detailed coverage files found, but coverage percentage extracted"
            touch backend-coverage-files.txt
          fi
        else
          echo "✅ Backend coverage meets threshold"
          echo "needs_more_tests=false" >> $GITHUB_OUTPUT
        fi

    # === AI-POWERED TEST FIXES & GENERATION ===
    - name: AI Fix Frontend Test Errors
      if: ${{ steps.analyze-frontend.outputs.needs_fixes == 'true' }}
      id: fix-frontend-tests
      continue-on-error: true
      run: |
        echo "🤖 Starting AI fixes for frontend tests"
        cd scripts
        
        echo "🔍 Phase 1: Prepare error context"
        cp ../scripts/frontend-test-errors.log enhanced-frontend-errors.log
        echo "   👉 Context size: $(wc -l < enhanced-frontend-errors.log) lines"
        
        echo "🧩 Phase 2: Junior AI propose fixes"
        echo "🔍 Starting Junior AI interactive exploration for frontend fixes..."
        python test-enhanced-proposer.py enhanced-frontend-errors.log \
          > frontend-proposed-fixes.json \
          2> >(tee frontend-proposer.log >&2)
        proposer_exit=$?
        echo ""
        echo "   ✅ Junior AI exit code: $proposer_exit"
        echo "   📄 Detailed exploration log saved to frontend-proposer.log"
        
        # Show preview of what Junior AI discovered
        if [ -f frontend-proposed-fixes.json ] && jq . frontend-proposed-fixes.json >/dev/null 2>&1; then
          proposed_count=$(jq -r '.changes | length' frontend-proposed-fixes.json 2>/dev/null || echo "0")
          echo "   📊 Proposed changes: $proposed_count"
          if [ "$proposed_count" -gt "0" ]; then
            echo "   🎯 Frontend fix preview:"
            jq -r '.changes[] | "      • \(.action) \(.file): \(.reasoning)"' frontend-proposed-fixes.json 2>/dev/null | head -3
          fi
        fi
        
        echo "🔍 Phase 3: Senior AI review"
        python review-changes.py frontend-proposed-fixes.json enhanced-frontend-errors.log \
          > frontend-reviewed-fixes.json 2> frontend-reviewer.log \
          || cp frontend-proposed-fixes.json frontend-reviewed-fixes.json
        reviewer_exit=$?
        echo "   ✅ Senior AI exit code: $reviewer_exit"
        
        echo "🔨 Phase 4: Apply changes"
        echo "📋 Checking input files for apply phase..."
        
        # Check if the input file exists and is valid
        if [ -f frontend-reviewed-fixes.json ]; then
          echo "   ✅ frontend-reviewed-fixes.json exists"
          echo "   📄 File size: $(wc -c < frontend-reviewed-fixes.json) bytes"
          echo "   📄 First 200 chars:"
          head -c 200 frontend-reviewed-fixes.json | sed 's/^/      /'
          echo ""
          
          # Validate JSON structure
          if jq . frontend-reviewed-fixes.json >/dev/null 2>&1; then
            echo "   ✅ JSON is valid"
            changes_count=$(jq -r '.changes | length' frontend-reviewed-fixes.json 2>/dev/null || echo "0")
            echo "   📊 Changes to apply: $changes_count"
            
            if [ "$changes_count" -gt "0" ]; then
              echo "   📋 Changes preview:"
              jq -r '.changes[] | "      • \(.action) \(.file)"' frontend-reviewed-fixes.json 2>/dev/null | head -5
            fi
          else
            echo "   ❌ JSON is invalid!"
            echo "   📄 Raw content (first 500 chars):"
            head -c 500 frontend-reviewed-fixes.json | sed 's/^/      /'
          fi
        else
          echo "   ❌ frontend-reviewed-fixes.json not found!"
          echo "   📋 Available files:"
          ls -la frontend-* | sed 's/^/      /' || echo "      No frontend-* files found"
        fi
        
        echo ""
        echo "🔧 Starting apply-changes.py..."
        echo "   📝 Command: python apply-changes.py frontend-reviewed-fixes.json"
        echo "   📂 Working directory: $(pwd)"
        echo "   🐍 Python version: $(python --version 2>&1 || echo 'Python not found')"
        echo "   🔍 Checking if apply-changes.py exists:"
        if [ -f apply-changes.py ]; then
          echo "      ✅ apply-changes.py found ($(wc -l < apply-changes.py) lines)"
        else
          echo "      ❌ apply-changes.py NOT FOUND!"
          ls -la *.py | head -5 | sed 's/^/         /'
        fi
        echo "   🔍 Python environment check:"
        echo "      VIRTUAL_ENV: ${VIRTUAL_ENV:-'Not set'}"
        echo "      Python path: $(which python 2>/dev/null || echo 'Not found')"
        echo "   🔍 File permissions check:"
        ls -la apply-changes.py frontend-reviewed-fixes.json 2>/dev/null | sed 's/^/      /' || echo "      Files not accessible"
        echo ""
        
        # Try to run apply-changes with maximum verbosity and capture everything
        echo "🚀 Executing apply-changes.py with full logging..."
        set -x  # Enable bash debug mode
        
        # Run with both stderr to file AND to console using tee
        python apply-changes.py frontend-reviewed-fixes.json > frontend-apply-result.json 2> >(tee frontend-applier.log >&2)
        apply_exit=$?
        set +x  # Disable bash debug mode
        
        # Also try direct execution with immediate stderr display if tee fails
        if [ $apply_exit -ne 0 ] && [ ! -s frontend-applier.log ]; then
          echo "⚠️ No error log captured, running again with direct stderr:"
          python apply-changes.py frontend-reviewed-fixes.json 2>&1 | tee direct-apply-output.log
        fi
        
        echo ""
        echo "🔍 Apply process completed with exit code: $apply_exit"
        echo "   📊 Process details:"
        echo "      Exit code: $apply_exit"
        echo "      Timestamp: $(date)"
        echo "      Process status: $(if [ $apply_exit -eq 0 ]; then echo 'SUCCESS'; else echo 'FAILED'; fi)"
        echo ""
        
        # Check what happened during apply
        echo "📄 Checking for error logs..."
        
        if [ -f frontend-applier.log ]; then
          echo "📄 Apply error log (frontend-applier.log):"
          if [ -s frontend-applier.log ]; then
            echo "   📊 Log size: $(wc -l < frontend-applier.log) lines"
            echo "   📋 Full error log:"
            cat frontend-applier.log | sed 's/^/      /'
          else
            echo "   ℹ️ Error log is empty"
          fi
        else
          echo "   ❌ No error log file created"
        fi
        
        # Check for direct output log
        if [ -f direct-apply-output.log ]; then
          echo ""
          echo "📄 Direct apply output log:"
          cat direct-apply-output.log | sed 's/^/      /'
        fi
        
        echo ""
        if [ -f frontend-apply-result.json ]; then
          echo "📄 Apply result (frontend-apply-result.json):"
          echo "   📊 File size: $(wc -c < frontend-apply-result.json) bytes"
          if [ -s frontend-apply-result.json ]; then
            echo "   📋 Content:"
            cat frontend-apply-result.json | sed 's/^/      /'
            
            # Try to parse result
            if jq . frontend-apply-result.json >/dev/null 2>&1; then
              applied=$(jq -r '.changes_applied // 0' frontend-apply-result.json 2>/dev/null || echo "0")
              total=$(jq -r '.total_changes // 0' frontend-apply-result.json 2>/dev/null || echo "0")
              echo "   📊 Parsed result: $applied/$total changes applied"
            else
              echo "   ⚠️ Result JSON is invalid"
            fi
          else
            echo "   ℹ️ Result file is empty"
          fi
        else
          echo "   ❌ No result file created"
        fi
        
        echo ""
        echo "🔍 Git status after apply attempt:"
        git status --porcelain | head -10 | sed 's/^/   /'
        if [ -z "$(git status --porcelain)" ]; then
          echo "   ℹ️ No git changes detected"
        fi
        
        echo "   ✅ Apply exit code: $apply_exit"
        
        # Safely extract applied count with error handling
        if [ -f frontend-apply-result.json ] && jq . frontend-apply-result.json >/dev/null 2>&1; then
          applied=$(jq -r '.changes_applied // 0' frontend-apply-result.json 2>/dev/null || echo "0")
        else
          applied="0"
          echo "   ⚠️ Warning: frontend-apply-result.json missing or invalid"
        fi
        echo "✅ Frontend fixes applied: $applied"
        echo "frontend_fixes_applied=$applied" >> $GITHUB_OUTPUT

    - name: AI Fix Backend Test Errors
      if: ${{ steps.analyze-backend.outputs.needs_fixes == 'true' }}
      id: fix-backend-tests
      run: |
        echo "🤖 Junior AI fixing backend test errors..."
        cd scripts
        
        # Enhance error log with context for Junior AI
        echo "=== BACKEND TEST ERROR ANALYSIS ===" > enhanced-backend-errors.log
        echo "Component: Backend (Node.js/TypeScript)" >> enhanced-backend-errors.log
        echo "Task: Setup test framework and create initial test suite if missing, or fix failing tests" >> enhanced-backend-errors.log
        echo "Context: Running 'bun run test --coverage' in backend/ directory" >> enhanced-backend-errors.log
        echo "" >> enhanced-backend-errors.log
        cat backend-test-errors.log >> enhanced-backend-errors.log
        
        echo "📝 Enhanced backend error log created with $(wc -l < enhanced-backend-errors.log) lines of context"
        
        # Phase 1: Junior AI Proposer - Backend Test Architecture & Fixes
        echo ""
        echo "🧠 PHASE 1: Junior AI Proposer Starting Backend Test Analysis..."
        echo "=============================="
        echo "🎯 Junior AI will:"
        echo "   • Analyze backend test framework requirements"
        echo "   • Explore existing API routes and business logic"
        echo "   • Design appropriate test architecture for Node.js/TypeScript"
        echo "   • Create test setup files and configuration if missing"
        echo "   • Generate API endpoint tests and unit tests"
        echo ""
        echo "🧠 Junior AI starting interactive exploration..."
        echo "🎯 Component: backend, Task: setup_tests_and_fix_failures"
        echo "🔍 Junior AI will use MCP tools to explore backend structure..."
        echo ""
        
        python test-enhanced-proposer.py enhanced-backend-errors.log \
          > backend-proposed-fixes.json \
          2> >(tee backend-proposer.log >&2)
        proposer_exit_code=$?
        
        echo ""
        if [ $proposer_exit_code -eq 0 ]; then
          if [ -f backend-proposed-fixes.json ] && jq . backend-proposed-fixes.json >/dev/null 2>&1; then
            proposed_count=$(jq -r '.changes | length' backend-proposed-fixes.json 2>/dev/null || echo "0")
            echo "✅ Junior AI Proposer completed backend analysis!"
            echo "   📊 Proposed changes: $proposed_count"
            echo "   📄 Detailed analysis saved to backend-proposer.log"
            echo "   🔍 Exploration steps logged in backend-proposer.log"
            
            if [ "$proposed_count" -gt "0" ]; then
              echo "   🎯 Backend change preview:"
              jq -r '.changes[] | "      • \(.action) \(.file): \(.reasoning)"' backend-proposed-fixes.json 2>/dev/null | head -5
            fi
          else
            echo "⚠️ Backend proposed fixes JSON missing or invalid"
          fi
        else
          echo "❌ Junior AI Proposer failed for backend with exit code: $proposer_exit_code"
          echo "📄 Exploration log:"
          tail -10 backend-proposer.log 2>/dev/null || echo "   No error log available"
        fi
        
        # Phase 2: Senior AI Reviewer - Backend-Specific Validation (Optional)
        echo ""
        echo "🎓 PHASE 2: Senior AI Reviewer Validating Backend Changes (Optional)..."
        echo "=============================="
        
        # First validate Junior AI output before sending to Senior AI
        echo "🔍 Validating Junior AI output quality..."
        if [ -f backend-proposed-fixes.json ] && jq . backend-proposed-fixes.json >/dev/null 2>&1; then
          junior_analysis=$(jq -r '.analysis // ""' backend-proposed-fixes.json 2>/dev/null || echo "")
          junior_changes_count=$(jq -r '.changes | length' backend-proposed-fixes.json 2>/dev/null || echo "0")
        else
          junior_analysis=""
          junior_changes_count="0"
          echo "⚠️ Backend proposed fixes JSON missing or invalid"
        fi
        
        # Check for error indicators in Junior AI analysis
        skip_senior_review=false
        if echo "$junior_analysis" | grep -iE "(error occurred|timeout|failed|max iterations|api call failed|all attempts|giving up)" > /dev/null; then
          echo "⚠️ Junior AI encountered issues, checking if output is still usable..."
          echo "   Analysis excerpt: $(echo "$junior_analysis" | head -c 100)..."
          
          if [ "$junior_changes_count" = "0" ] || [ "$junior_changes_count" = "null" ]; then
            echo "❌ Junior AI failed with no usable changes, skipping Senior AI review"
            skip_senior_review=true
          else
            echo "✅ Junior AI provided $junior_changes_count changes despite issues, proceeding with review"
          fi
        else
          echo "✅ Junior AI output looks good, proceeding with Senior AI review"
        fi
        
        if [ "$skip_senior_review" = "false" ]; then
          echo "🔍 Senior AI will validate:"
          echo "   • API testing patterns and best practices"
          echo "   • Test database setup and teardown procedures"
          echo "   • Mocking strategies for external dependencies"
          echo "   • Integration test coverage for critical paths"
          echo "   • Performance test considerations"
          echo "   • If review fails, we'll continue with original proposals"
          
          python review-changes.py backend-proposed-fixes.json enhanced-backend-errors.log > backend-reviewed-fixes.json 2> backend-reviewer.log
          reviewer_exit_code=$?
        else
          echo "🚫 Skipping Senior AI review due to Junior AI failure"
          reviewer_exit_code=1  # Set to failed to use original proposals
        fi
        
        # Set the file to use for application (reviewed if successful, original if failed)
        backend_final_fixes_file="backend-proposed-fixes.json"
        
        echo ""
        if [ $reviewer_exit_code -eq 0 ]; then
          if [ -f backend-reviewed-fixes.json ] && jq . backend-reviewed-fixes.json >/dev/null 2>&1; then
            reviewed_count=$(jq -r '.changes | length' backend-reviewed-fixes.json 2>/dev/null || echo "0")
            if [ "$reviewed_count" -gt "0" ]; then
              echo "✅ Senior AI Reviewer completed backend validation!"
              echo "   📊 Reviewed changes: $reviewed_count"
              echo "   📄 Detailed review saved to backend-reviewer.log"
              backend_final_fixes_file="backend-reviewed-fixes.json"
            else
              echo "⚠️ Senior AI review produced empty changes, using original backend proposals"
            fi
          else
            echo "⚠️ Senior AI completed but no reviewed fixes file generated, using original backend proposals"
          fi
        else
          echo "❌ Senior AI Reviewer failed for backend with exit code: $reviewer_exit_code"
          echo "📄 Error details:"
          tail -10 backend-reviewer.log 2>/dev/null || echo "   No error log available"
          echo "🔄 Continuing with original Junior AI backend proposals..."
        fi
        
        echo "📋 Will apply backend changes from: $backend_final_fixes_file"
        
        # Phase 3: Automated Applier - Backend Implementation
        echo ""
        echo "🔧 PHASE 3: Applying Backend Test Infrastructure..."
        echo "=============================="
        echo "📋 Backend apply command: python apply-changes.py $backend_final_fixes_file"
        echo "📂 Working directory: $(pwd)"
        
        # Enhanced logging for backend apply
        echo "🔍 Checking backend input file:"
        if [ -f "$backend_final_fixes_file" ]; then
          echo "   ✅ $backend_final_fixes_file exists"
          echo "   📄 File size: $(wc -c < "$backend_final_fixes_file") bytes"
          if jq . "$backend_final_fixes_file" >/dev/null 2>&1; then
            changes_count=$(jq -r '.changes | length' "$backend_final_fixes_file" 2>/dev/null || echo "0")
            echo "   ✅ JSON is valid with $changes_count changes"
          else
            echo "   ❌ JSON is invalid!"
          fi
        else
          echo "   ❌ $backend_final_fixes_file not found!"
        fi
        
        echo ""
        echo "🚀 Executing backend apply-changes.py with full logging..."
        set -x  # Enable bash debug mode
        
        # Run with both stderr to file AND to console using tee
        python apply-changes.py "$backend_final_fixes_file" > backend-apply-result.json 2> >(tee backend-applier.log >&2)
        applier_exit_code=$?
        set +x  # Disable bash debug mode
        
        # Also try direct execution with immediate stderr display if tee fails
        if [ $applier_exit_code -ne 0 ] && [ ! -s backend-applier.log ]; then
          echo "⚠️ No backend error log captured, running again with direct stderr:"
          python apply-changes.py "$backend_final_fixes_file" 2>&1 | tee backend-direct-apply-output.log
        fi
        
        echo ""
        echo "🔍 Backend apply process completed with exit code: $applier_exit_code"
        
        # Check what happened during backend apply
        echo "📄 Checking for backend error logs..."
        
        if [ -f backend-applier.log ]; then
          echo "📄 Backend apply error log (backend-applier.log):"
          if [ -s backend-applier.log ]; then
            echo "   📊 Log size: $(wc -l < backend-applier.log) lines"
            echo "   📋 Full error log:"
            cat backend-applier.log | sed 's/^/      /'
          else
            echo "   ℹ️ Backend error log is empty"
          fi
        else
          echo "   ❌ No backend error log file created"
        fi
        
        # Check for direct output log
        if [ -f backend-direct-apply-output.log ]; then
          echo ""
          echo "📄 Backend direct apply output log:"
          cat backend-direct-apply-output.log | sed 's/^/      /'
        fi
        
        echo ""
        if [ $applier_exit_code -eq 0 ]; then
          # Safely extract backend apply results with error handling
          if [ -f backend-apply-result.json ] && jq . backend-apply-result.json >/dev/null 2>&1; then
            applied=$(jq -r '.changes_applied // 0' backend-apply-result.json 2>/dev/null || echo "0")
            total=$(jq -r '.total_changes // 0' backend-apply-result.json 2>/dev/null || echo "0")
          else
            applied="0"
            total="0"
            echo "   ⚠️ Warning: backend-apply-result.json missing or invalid"
          fi
          
          echo "✅ Backend test infrastructure applied successfully!"
          echo "   📊 Changes applied: $applied/$total"
          echo "   📄 Detailed application log saved to backend-applier.log"
            
          # ADD DETAILED GIT PUSH LOGGING FOR BACKEND
          echo ""
          echo "🔄 BACKEND GIT COMMIT AND PUSH STATUS:"
          echo "=============================="
          
          if [ "$applied" -gt "0" ]; then
            echo "📋 Checking git status after backend changes..."
            git status --porcelain
            
            echo "📝 Checking recent commits..."
            git log --oneline -3
            
            echo "🌐 Checking if backend changes were pushed to remote..."
            current_branch=$(git rev-parse --abbrev-ref HEAD)
            local_commits=$(git rev-list --count HEAD ^origin/$current_branch 2>/dev/null || echo "0")
            echo "   Local commits ahead of remote: $local_commits"
            
            if [ "$local_commits" -gt "0" ]; then
              echo "   ⚠️ WARNING: Backend changes were NOT pushed to remote!"
            else
              echo "   ✅ Backend changes were pushed successfully"
            fi
          fi
          
          echo "backend_fixes_applied=$applied" >> $GITHUB_OUTPUT
          echo ""
          echo "🎉 BACKEND TEST SETUP COMPLETE: $applied changes successfully applied!"
        else
          echo "❌ Backend applier failed with exit code: $applier_exit_code"
          echo "📄 Backend failure analysis:"
          
          # Display error logs if available
          if [ -f backend-applier.log ] && [ -s backend-applier.log ]; then
            echo "   📋 Error log (last 20 lines):"
            tail -20 backend-applier.log | sed 's/^/      /'
          elif [ -f backend-direct-apply-output.log ]; then
            echo "   📋 Direct output log (last 20 lines):"
            tail -20 backend-direct-apply-output.log | sed 's/^/      /'
          else
            echo "   ❌ No error logs available"
          fi
          
          echo "backend_fixes_applied=0" >> $GITHUB_OUTPUT
        fi

    - name: AI Generate Missing Frontend Tests
      if: ${{ (inputs.add_missing_tests == true || inputs.add_missing_tests == null) && steps.analyze-frontend.outputs.needs_more_tests == 'true' }}
      id: generate-frontend-tests
      run: |
        echo "🤖 Junior AI generating missing frontend tests based on coverage..."
        cd scripts
        
        # Create comprehensive coverage analysis for Junior AI
        echo "=== FRONTEND TEST COVERAGE ENHANCEMENT ===" > coverage-analysis.log
        echo "Component: Frontend (React/TypeScript)" >> coverage-analysis.log
        echo "Task: Generate missing tests to improve code coverage" >> coverage-analysis.log
        echo "Current coverage: ${{ steps.analyze-frontend.outputs.coverage_percentage }}%" >> coverage-analysis.log
        echo "Target coverage: ${{ inputs.coverage_threshold || 80 }}%" >> coverage-analysis.log
        echo "Framework: Vitest + Testing Library + Jest DOM" >> coverage-analysis.log
        echo "" >> coverage-analysis.log
        echo "COVERAGE GAPS TO ADDRESS:" >> coverage-analysis.log
        echo "- Create tests for untested components" >> coverage-analysis.log
        echo "- Add edge case scenarios" >> coverage-analysis.log
        echo "- Test error handling paths" >> coverage-analysis.log
        echo "- Add integration tests where needed" >> coverage-analysis.log
        echo "" >> coverage-analysis.log
        
        # Add uncovered files information if available
        if [ -f ../ui/coverage-files.txt ]; then
          echo "UNCOVERED/LOW COVERAGE FILES:" >> coverage-analysis.log
          while IFS= read -r file; do
            if [ -f "$file" ]; then
              echo "File: $file" >> coverage-analysis.log
              # Extract uncovered lines info if available in JSON format
              if command -v jq >/dev/null 2>&1; then
                jq -r '.uncoveredLines // []' "$file" 2>/dev/null | head -10 >> coverage-analysis.log || true
              fi
            fi
          done < ../ui/coverage-files.txt
        fi
        
        # Add examples of existing test patterns for context
        echo "" >> coverage-analysis.log
        echo "EXISTING TEST PATTERNS (for reference):" >> coverage-analysis.log
        find ../ui/test -name "*.test.tsx" -o -name "*.test.ts" 2>/dev/null | head -3 | while read test_file; do
          echo "Example test file: $test_file" >> coverage-analysis.log
          head -20 "$test_file" 2>/dev/null >> coverage-analysis.log || true
          echo "---" >> coverage-analysis.log
        done
        
        echo "🧠 Junior AI starting interactive coverage exploration..."
        echo "🎯 Component: frontend, Task: coverage_improvement"
        echo "📊 Current coverage: ${{ steps.analyze-frontend.outputs.coverage_percentage }}%"
        echo "🎯 Target coverage: ${{ inputs.coverage_threshold || 80 }}%"
        echo "🔍 Junior AI will explore:"
        echo "   • Analyze existing test patterns and structure"
        echo "   • Identify untested components and utilities"
        echo "   • Examine uncovered code paths and edge cases"
        echo "   • Design comprehensive test suites for gaps"
        echo "   • Create integration tests for component interactions"
        echo ""
        
        # Use Junior AI to generate comprehensive test coverage improvements
        # Run with stderr piped through tee so we see MCP logs live
        python test-enhanced-proposer.py coverage-analysis.log \
          > frontend-test-proposals.json \
          2> >(tee frontend-coverage-proposer.log >&2)
        
        echo ""
        echo "� Junior AI Exploration Summary (last 20 lines):"
        if [ -f frontend-coverage-proposer.log ]; then
          tail -20 frontend-coverage-proposer.log | sed 's/^/   /'
        fi
        echo ""
        echo "   📄 Full exploration log saved to frontend-coverage-proposer.log"
        
        # Show what Junior AI discovered
        if [ -f frontend-test-proposals.json ] && jq . frontend-test-proposals.json >/dev/null 2>&1; then
          proposed_count=$(jq -r '.changes | length' frontend-test-proposals.json 2>/dev/null || echo "0")
          echo "   📊 Proposed test improvements: $proposed_count"
          if [ "$proposed_count" -gt "0" ]; then
            echo "   🎯 Coverage improvement preview:"
            jq -r '.changes[] | "      • \(.action) \(.file): \(.reasoning)"' frontend-test-proposals.json 2>/dev/null | head -3
          fi
        fi
        
        python review-changes.py frontend-test-proposals.json coverage-analysis.log > frontend-test-reviewed.json 2> frontend-coverage-reviewer.log
        python apply-changes.py frontend-test-reviewed.json > frontend-test-result.json 2> frontend-coverage-applier.log
        
        # Check results
        if [ -f frontend-test-result.json ]; then
          echo "📄 Checking frontend-test-result.json for valid JSON..."
          if jq . frontend-test-result.json >/dev/null 2>&1; then
            added=$(cat frontend-test-result.json | jq -r '.changes_applied // 0' 2>/dev/null || echo "0")
            echo "frontend_tests_added=$added" >> $GITHUB_OUTPUT
            echo "📝 Generated $added frontend test files/improvements"
          else
            echo "⚠️ frontend-test-result.json contains invalid JSON, using fallback"
            echo "frontend_tests_added=0" >> $GITHUB_OUTPUT
            echo "📄 Raw content preview:"
            head -3 frontend-test-result.json | sed 's/^/   /'
          fi
        else
          echo "❌ frontend-test-result.json not found"
          echo "frontend_tests_added=0" >> $GITHUB_OUTPUT
        fi

    - name: AI Generate Missing Backend Tests
      if: ${{ (inputs.add_missing_tests == true || inputs.add_missing_tests == null) && steps.analyze-backend.outputs.needs_more_tests == 'true' }}
      id: generate-backend-tests
      run: |
        echo "🤖 Junior AI generating missing backend tests based on coverage..."
        cd scripts
        
        # Create comprehensive coverage analysis for Junior AI
        echo "=== BACKEND TEST COVERAGE ENHANCEMENT ===" > backend-coverage-analysis.log
        echo "Component: Backend (Node.js/TypeScript)" >> backend-coverage-analysis.log
        echo "Task: Generate missing tests to improve code coverage" >> backend-coverage-analysis.log
        echo "Current coverage: ${{ steps.analyze-backend.outputs.coverage_percentage }}%" >> backend-coverage-analysis.log
        echo "Target coverage: ${{ inputs.coverage_threshold || 80 }}%" >> backend-coverage-analysis.log
        echo "Framework: Bun Test + Elysia + Eden Treaty" >> backend-coverage-analysis.log
        echo "" >> backend-coverage-analysis.log
        echo "COVERAGE GAPS TO ADDRESS:" >> backend-coverage-analysis.log
        echo "- Create tests for untested API routes" >> backend-coverage-analysis.log
        echo "- Add edge case scenarios" >> backend-coverage-analysis.log
        echo "- Test error handling paths" >> backend-coverage-analysis.log
        echo "- Add integration tests for API endpoints" >> backend-coverage-analysis.log
        echo "- Test middleware and validation logic" >> backend-coverage-analysis.log
        echo "" >> backend-coverage-analysis.log
        
        # Add uncovered files information if available
        if [ -f ../backend/backend-coverage-files.txt ]; then
          echo "UNCOVERED/LOW COVERAGE FILES:" >> backend-coverage-analysis.log
          while IFS= read -r file; do
            if [ -f "$file" ]; then
              echo "File: $file" >> backend-coverage-analysis.log
              # Extract uncovered lines info if available in JSON format
              if command -v jq >/dev/null 2>&1; then
                jq -r '.uncoveredLines // []' "$file" 2>/dev/null | head -10 >> backend-coverage-analysis.log || true
              fi
            fi
          done < ../backend/backend-coverage-files.txt
        fi
        
        # Add examples of existing test patterns for context
        echo "" >> backend-coverage-analysis.log
        echo "EXISTING TEST PATTERNS (for reference):" >> backend-coverage-analysis.log
        find ../backend/test -name "*.test.ts" -o -name "*.test.js" 2>/dev/null | head -3 | while read test_file; do
          echo "Example test file: $test_file" >> backend-coverage-analysis.log
          head -20 "$test_file" 2>/dev/null >> backend-coverage-analysis.log || true
          echo "---" >> backend-coverage-analysis.log
        done
        
        echo "🧠 Junior AI starting interactive backend coverage exploration..."
        echo "🎯 Component: backend, Task: coverage_improvement"
        echo "📊 Current coverage: ${{ steps.analyze-backend.outputs.coverage_percentage }}%"
        echo "🎯 Target coverage: ${{ inputs.coverage_threshold || 80 }}%"
        echo "🔍 Junior AI will explore:"
        echo "   • Analyze existing backend test patterns and structure"
        echo "   • Identify untested API routes and business logic"
        echo "   • Examine uncovered code paths and edge cases"
        echo "   • Design comprehensive test suites for API endpoints"
        echo "   • Create integration tests for route interactions"
        echo ""
        
        # Use Junior AI to generate comprehensive backend test coverage improvements
        echo "📋 Starting Junior AI interactive exploration..."
        echo "🔍 Junior AI will use MCP tools to:"
        echo "   • Explore backend codebase structure and existing tests"
        echo "   • Analyze uncovered routes and utilities"
        echo "   • Design targeted backend test improvements"
        echo ""
        
        # Run with stderr piped through tee so we see MCP logs live
        python test-enhanced-proposer.py backend-coverage-analysis.log \
          > backend-test-proposals.json \
          2> >(tee backend-coverage-proposer.log >&2)
        
        echo ""
        echo "📜 Junior AI Backend Exploration Summary (last 20 lines):"
        if [ -f backend-coverage-proposer.log ]; then
          tail -20 backend-coverage-proposer.log | sed 's/^/   /'
        fi
        echo ""
        echo "   📄 Full exploration log saved to backend-coverage-proposer.log"
        
        # Show what Junior AI discovered
        if [ -f backend-test-proposals.json ] && jq . backend-test-proposals.json >/dev/null 2>&1; then
          proposed_count=$(jq -r '.changes | length' backend-test-proposals.json 2>/dev/null || echo "0")
          echo "   📊 Proposed backend test improvements: $proposed_count"
          if [ "$proposed_count" -gt "0" ]; then
            echo "   🎯 Backend coverage improvement preview:"
            jq -r '.changes[] | "      • \(.action) \(.file): \(.reasoning)"' backend-test-proposals.json 2>/dev/null | head -3
          fi
        fi
        
        python review-changes.py backend-test-proposals.json backend-coverage-analysis.log > backend-test-reviewed.json 2> backend-coverage-reviewer.log
        python apply-changes.py backend-test-reviewed.json > backend-test-result.json 2> backend-coverage-applier.log
        
        # Check results
        if [ -f backend-test-result.json ]; then
          echo "📄 Checking backend-test-result.json for valid JSON..."
          if jq . backend-test-result.json >/dev/null 2>&1; then
            added=$(cat backend-test-result.json | jq -r '.changes_applied // 0' 2>/dev/null || echo "0")
            echo "backend_tests_added=$added" >> $GITHUB_OUTPUT
            echo "📝 Generated $added backend test files/improvements"
          else
            echo "⚠️ backend-test-result.json contains invalid JSON, using fallback"
            echo "backend_tests_added=0" >> $GITHUB_OUTPUT
            echo "📄 Raw content preview:"
            head -3 backend-test-result.json | sed 's/^/   /'
          fi
        else
          echo "❌ backend-test-result.json not found"
          echo "backend_tests_added=0" >> $GITHUB_OUTPUT
        fi

    - name: Re-run Tests After AI Fixes
      if: ${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied > 0 || steps.fix-backend-tests.outputs.backend_fixes_applied > 0 || steps.generate-frontend-tests.outputs.frontend_tests_added > 0 || steps.generate-backend-tests.outputs.backend_tests_added > 0 }}
      id: rerun-tests
      run: |
        echo "🔄 Re-running tests after AI improvements..."
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        
        # Re-run frontend tests if they were fixed
        if [ "${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied || 0 }}" -gt "0" ] || [ "${{ steps.generate-frontend-tests.outputs.frontend_tests_added || 0 }}" -gt "0" ]; then
          echo "🧪 Re-running frontend tests..."
          echo "🔍 Changes applied:"
          echo "   • Test fixes: ${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied || 0 }}"
          echo "   • New tests: ${{ steps.generate-frontend-tests.outputs.frontend_tests_added || 0 }}"
          echo ""
          echo "📊 Running frontend test suite with coverage..."
          cd ui
          bun run test:coverage --reporter=verbose 2>&1 | tee ../scripts/frontend-retest.log
          frontend_retest_exit=$?
          echo "frontend_retest_exit_code=$frontend_retest_exit" >> $GITHUB_OUTPUT
          
          echo ""
          if [ $frontend_retest_exit -eq 0 ]; then
            echo "✅ Frontend tests passed after AI improvements!"
          else
            echo "❌ Frontend tests still failing after AI improvements"
            echo "📄 Last 10 lines of test output:"
            tail -10 ../scripts/frontend-retest.log | sed 's/^/   /'
          fi
          
          # Check new coverage
          if [ -f coverage/coverage-summary.json ]; then
            new_coverage=$(cat coverage/coverage-summary.json | jq -r '.total.lines.pct // 0')
            echo "frontend_new_coverage=$new_coverage" >> $GITHUB_OUTPUT
            echo "📊 New frontend coverage: $new_coverage%"
            
            # Compare with old coverage
            old_coverage="${{ steps.analyze-frontend.outputs.coverage_percentage || 0 }}"
            echo "📈 Coverage change: $old_coverage% → $new_coverage%"
          fi
          cd ..
        fi
        
        # Re-run backend tests if they were fixed or new tests were added
        if [ "${{ steps.fix-backend-tests.outputs.backend_fixes_applied || 0 }}" -gt "0" ] || [ "${{ steps.generate-backend-tests.outputs.backend_tests_added || 0 }}" -gt "0" ]; then
          echo ""
          echo "🧪 Re-running backend tests..."
          echo "🔍 Changes applied:"
          echo "   • Test fixes: ${{ steps.fix-backend-tests.outputs.backend_fixes_applied || 0 }}"
          echo "   • New tests: ${{ steps.generate-backend-tests.outputs.backend_tests_added || 0 }}"
          echo ""
          echo "📊 Running backend test suite with coverage..."
          cd backend
          bun run test --coverage 2>&1 | tee ../scripts/backend-retest.log
          backend_retest_exit=$?
          echo "backend_retest_exit_code=$backend_retest_exit" >> $GITHUB_OUTPUT
          
          echo ""
          if [ $backend_retest_exit -eq 0 ]; then
            echo "✅ Backend tests passed after AI improvements!"
          else
            echo "❌ Backend tests still failing after AI improvements"
            echo "📄 Last 10 lines of test output:"
            tail -10 ../scripts/backend-retest.log | sed 's/^/   /'
          fi
          
          # Check new backend coverage
          if [ -f coverage/coverage-summary.json ]; then
            new_coverage=$(cat coverage/coverage-summary.json | jq -r '.total.lines.pct // 0')
            echo "backend_new_coverage=$new_coverage" >> $GITHUB_OUTPUT
            echo "📊 New backend coverage: $new_coverage%"
            
            # Compare with old coverage
            old_coverage="${{ steps.analyze-backend.outputs.coverage_percentage || 0 }}"
            echo "📈 Backend coverage change: $old_coverage% → $new_coverage%"
          fi
          cd ..
        fi
        
        echo ""
        echo "🎉 Test re-run completed!"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

    - name: Generate Test Summary
      id: summary
      run: |
        echo "📊 Generating test improvement summary..."
        
        # Calculate totals safely
        frontend_fixes="${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied || 0 }}"
        backend_fixes="${{ steps.fix-backend-tests.outputs.backend_fixes_applied || 0 }}"
        frontend_tests="${{ steps.generate-frontend-tests.outputs.frontend_tests_added || 0 }}"
        backend_tests="${{ steps.generate-backend-tests.outputs.backend_tests_added || 0 }}"
        
        total_fixes=$((frontend_fixes + backend_fixes))
        total_tests_added=$((frontend_tests + backend_tests))
        
        echo "tests_added=$total_tests_added" >> $GITHUB_OUTPUT
        echo "errors_fixed=$total_fixes" >> $GITHUB_OUTPUT
        
        # Frontend coverage (with proper fallbacks)
        frontend_old_coverage="${{ steps.analyze-frontend.outputs.coverage_percentage || 0 }}"
        frontend_new_coverage="${{ steps.rerun-tests.outputs.frontend_new_coverage }}"
        
        # If no new coverage measured, use old coverage
        if [ -z "$frontend_new_coverage" ] || [ "$frontend_new_coverage" = "0" ]; then
          frontend_new_coverage="$frontend_old_coverage"
        fi
        
        # Backend coverage (with proper fallbacks)
        backend_old_coverage="${{ steps.analyze-backend.outputs.coverage_percentage || 0 }}"
        backend_new_coverage="${{ steps.rerun-tests.outputs.backend_new_coverage }}"
        
        # If no new coverage measured, use old coverage
        if [ -z "$backend_new_coverage" ] || [ "$backend_new_coverage" = "0" ]; then
          backend_new_coverage="$backend_old_coverage"
        fi
        
        # Safe coverage comparison (avoid bc dependency)
        coverage_improved="false"
        if [ "$frontend_new_coverage" != "$frontend_old_coverage" ] || [ "$backend_new_coverage" != "$backend_old_coverage" ]; then
          coverage_improved="true"
          echo "📈 Coverage changes detected"
          echo "   Frontend: $frontend_old_coverage% → $frontend_new_coverage%"
          echo "   Backend: $backend_old_coverage% → $backend_new_coverage%"
        fi
        echo "coverage_improved=$coverage_improved" >> $GITHUB_OUTPUT
        
        # Enhanced test status detection
        frontend_status="ℹ️ No changes"
        backend_status="ℹ️ No changes" 
        
        # Frontend status
        if [ "$frontend_fixes" -gt "0" ] || [ "$frontend_tests" -gt "0" ]; then
          frontend_exit="${{ steps.rerun-tests.outputs.frontend_retest_exit_code }}"
          if [ "$frontend_exit" = "0" ]; then
            frontend_status="✅ Tests passing"
          elif [ -n "$frontend_exit" ]; then
            frontend_status="❌ Tests failing"
          else
            frontend_status="⚠️ Not re-tested"
          fi
        fi
        
        # Backend status with annotation check
        if [ "$backend_fixes" -gt "0" ] || [ "$backend_tests" -gt "0" ]; then
          backend_exit="${{ steps.rerun-tests.outputs.backend_retest_exit_code }}"
          
          # Check for test failures in logs (more accurate than exit code)
          test_failures="0"
          if [ -f ../scripts/backend-retest.log ]; then
            # Count expect().toBe() failures
            test_failures=$(grep -c "expect.*toBe.*Expected.*Received" ../scripts/backend-retest.log 2>/dev/null || echo "0")
          fi
          
          if [ "$test_failures" -gt "0" ]; then
            backend_status="❌ $test_failures test failures (500 errors)"
          elif [ "$backend_exit" = "0" ]; then
            backend_status="✅ Tests passing"
          elif [ -n "$backend_exit" ]; then
            backend_status="❌ Tests failing"
          else
            backend_status="⚠️ Not re-tested"
          fi
        fi
        
        # Display values for debugging
        echo "🔍 Debug values:"
        echo "   Frontend old/new: $frontend_old_coverage% / $frontend_new_coverage%"
        echo "   Backend old/new: $backend_old_coverage% / $backend_new_coverage%"
        echo "   Frontend status: $frontend_status"
        echo "   Backend status: $backend_status"
        
        # Create enhanced summary
        cat > test-summary.md << EOF
        ## 🤖 AI Test Builder Results
        
        ### 📊 Summary
        - **Errors Fixed**: $total_fixes
        - **Tests Added**: $total_tests_added (Frontend: $frontend_tests, Backend: $backend_tests)
        - **Frontend Coverage**: $frontend_old_coverage% → $frontend_new_coverage%
        - **Backend Coverage**: $backend_old_coverage% → $backend_new_coverage%
        
        ### 🎯 Frontend Results
        - Initial Coverage: $frontend_old_coverage%
        - Fixes Applied: $frontend_fixes
        - Tests Added: $frontend_tests
        - Final Coverage: $frontend_new_coverage%
        - Tests Status: $frontend_status
        
        ### 🔧 Backend Results  
        - Initial Coverage: $backend_old_coverage%
        - Fixes Applied: $backend_fixes
        - Tests Added: $backend_tests
        - Final Coverage: $backend_new_coverage%
        - Tests Status: $backend_status
        
        ### ⚠️ Issues Detected
        EOF
        
        # Add specific issues found
        if [ "$test_failures" -gt "0" ]; then
          cat >> test-summary.md << EOF
        - Backend auth middleware tests failing with 500 errors instead of expected 401/200
        - Generated tests may not match actual API behavior
        - API setup or middleware configuration needs investigation
        EOF
        else
          echo "- No critical test failures detected" >> test-summary.md
        fi
        
        cat >> test-summary.md << EOF
        
        ### 📁 Files Modified
        Check the commit history for detailed changes made by the AI.
        
        ### 🎯 Next Steps
        - Fix backend auth middleware 500 errors if present
        - Review AI-generated test expectations vs actual API behavior
        - Consider API documentation updates for accurate test generation
        EOF
        
        echo "📋 Enhanced test summary generated"

    - name: Upload Test Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ai-test-builder-reports
        path: |
          scripts/*.log
          scripts/*.json
          ui/coverage/
          backend/coverage/
          test-summary.md
        retention-days: 30

    - name: Comment on PR
      if: ${{ github.event_name == 'push' }}
      continue-on-error: true
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          try {
            if (fs.existsSync('test-summary.md')) {
              const summary = fs.readFileSync('test-summary.md', 'utf8');
              
              // Try to find an open PR for this branch
              const prs = await github.rest.pulls.list({
                owner: context.repo.owner,
                repo: context.repo.repo,
                head: `${context.repo.owner}:${context.ref.replace('refs/heads/', '')}`,
                state: 'open'
              });
              
              if (prs.data.length > 0) {
                console.log(`📝 Attempting to comment on PR #${prs.data[0].number}`);
                await github.rest.issues.createComment({
                  issue_number: prs.data[0].number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: summary
                });
                console.log(`✅ Successfully commented on PR #${prs.data[0].number}`);
              } else {
                console.log('ℹ️ No open PR found for this branch');
              }
            } else {
              console.log('ℹ️ No test summary file found');
            }
          } catch (error) {
            if (error.status === 403) {
              console.log('⚠️ Permission denied: GitHub App does not have permission to comment on PRs');
              console.log('💡 Consider adding "pull-requests: write" permission to the workflow');
            } else {
              console.log(`❌ Failed to comment on PR: ${error.message}`);
            }
            // Don't fail the workflow for comment issues
          }

    - name: Create Summary
      if: always()
      run: |
        echo "## 🤖 AI Test Builder Completed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f test-summary.md ]; then
          cat test-summary.md >> $GITHUB_STEP_SUMMARY
        else
          echo "No detailed summary available" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🎯 Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- Review the AI-generated tests and fixes" >> $GITHUB_STEP_SUMMARY
        echo "- Merge changes if tests are passing and coverage improved" >> $GITHUB_STEP_SUMMARY
        echo "- Consider adjusting coverage thresholds if needed" >> $GITHUB_STEP_SUMMARY
