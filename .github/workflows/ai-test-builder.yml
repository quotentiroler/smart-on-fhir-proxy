name: AI Test Builder

on:
  push:
    branches:
      - "ai/test/**"
  workflow_dispatch:
    inputs:
      target_component:
        description: "Target component to test (frontend/backend/both)"
        required: false
        type: choice
        options:
          - both
          - frontend
          - backend
        default: both
      coverage_threshold:
        description: "Minimum coverage threshold (%)"
        required: false
        type: number
        default: 80
      add_missing_tests:
        description: "Add missing tests based on coverage"
        required: false
        type: boolean
        default: true

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  ai-test-builder:
    name: AI Test Builder & Fixer
    runs-on: ubuntu-latest
    
    outputs:
      tests_added: ${{ steps.summary.outputs.tests_added }}
      coverage_improved: ${{ steps.summary.outputs.coverage_improved }}
      errors_fixed: ${{ steps.summary.outputs.errors_fixed }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for coverage comparison

    - name: Setup Bun
      uses: ./.github/actions/setup-bun-version

    - name: Setup Python with UV for AI Scripts
      uses: ./.github/actions/setup-python-ai
      with:
        python-version: '3.11'
        cache-dependency-path: 'scripts/requirements.txt'

    # Install Dependencies
    - name: Install Root Dependencies
      run: |
        echo "ğŸ“¦ Installing root dependencies..."
        bun install

    - name: Install Backend Dependencies
      run: |
        echo "ğŸ“¦ Installing backend dependencies..."
        cd backend
        bun install

    - name: Install Frontend Dependencies
      run: |
        echo "ğŸ“¦ Installing frontend dependencies..."
        cd ui
        bun install

    # === FRONTEND TESTING & COVERAGE ===
    - name: Run Frontend Tests with Coverage
      if: ${{ inputs.target_component == 'frontend' || inputs.target_component == 'both' || inputs.target_component == null }}
      id: frontend-test-coverage
      continue-on-error: true
      run: |
        echo "ğŸ§ª Running frontend tests with coverage..."
        cd ui
        
        # Run tests with coverage
        bun run test:coverage --reporter=verbose --reporter=json --outputFile=test-results.json 2>&1 | tee test-output.log
        
        # Capture exit code
        echo "frontend_test_exit_code=$?" >> $GITHUB_OUTPUT
        
        # Generate coverage report
        if [ -f coverage/coverage-summary.json ]; then
          echo "ğŸ“Š Coverage report generated"
          cat coverage/coverage-summary.json
        else
          echo "âš ï¸ No coverage report found"
        fi

    - name: Analyze Frontend Test Results
      if: ${{ (inputs.target_component == 'frontend' || inputs.target_component == 'both' || inputs.target_component == null) && steps.frontend-test-coverage.outcome != 'skipped' }}
      id: analyze-frontend
      run: |
        echo "ğŸ” Analyzing frontend test results..."
        cd ui
        
        # Check if tests failed (exit code OR output contains failures)
        test_exit_code="${{ steps.frontend-test-coverage.outputs.frontend_test_exit_code }}"
        has_test_failures="false"
        
        # Check exit code
        if [ "$test_exit_code" != "0" ]; then
          echo "âŒ Frontend tests failed with exit code: $test_exit_code"
          has_test_failures="true"
        fi
        
        # Also check test output for failure indicators
        if [ -f test-output.log ]; then
          if grep -qE "(failed|error:|FAIL|AssertionError)" test-output.log; then
            echo "âŒ Frontend test output contains failures/errors"
            has_test_failures="true"
          fi
        fi
        
        if [ "$has_test_failures" = "true" ]; then
          echo "needs_fixes=true" >> $GITHUB_OUTPUT
          
          # Save error log for AI analysis
          cp test-output.log ../scripts/frontend-test-errors.log
        else
          echo "âœ… Frontend tests passed"
          echo "needs_fixes=false" >> $GITHUB_OUTPUT
        fi
        
        # Analyze coverage
        if [ -f coverage/coverage-summary.json ]; then
          # Extract overall coverage percentage
          coverage_pct=$(cat coverage/coverage-summary.json | jq -r '.total.lines.pct // 0')
          echo "coverage_percentage=$coverage_pct" >> $GITHUB_OUTPUT
          echo "ğŸ“Š Frontend coverage: $coverage_pct%"
          
          # Check if below threshold
          threshold="${{ inputs.coverage_threshold || 80 }}"
          if (( $(echo "$coverage_pct < $threshold" | bc -l) )); then
            echo "ğŸ“‰ Coverage below threshold ($threshold%), will add tests"
            echo "needs_more_tests=true" >> $GITHUB_OUTPUT
            
            # Generate uncovered files report
            find coverage -name "*.json" -not -name "coverage-summary.json" | head -20 > coverage-files.txt
          else
            echo "âœ… Coverage meets threshold"
            echo "needs_more_tests=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "needs_more_tests=true" >> $GITHUB_OUTPUT
        fi

    # === BACKEND TESTING & COVERAGE ===
    - name: Run Backend Tests with Coverage
      if: ${{ inputs.target_component == 'backend' || inputs.target_component == 'both' || inputs.target_component == null }}
      id: backend-test-coverage
      continue-on-error: true
      run: |
        echo "ğŸ§ª Running backend tests with coverage..."
        cd backend
        
        # Check if backend has tests
        if [ ! -d "test" ] && [ ! -f "package.json" ] || ! grep -q '"test"' package.json; then
          echo "âš ï¸ No backend tests found, will create test setup"
          echo "backend_test_exit_code=1" >> $GITHUB_OUTPUT
          echo "No backend tests configured" > test-output.log
        else
          # Run tests with coverage
          bun run test --coverage 2>&1 | tee test-output.log
          echo "backend_test_exit_code=$?" >> $GITHUB_OUTPUT
        fi

    - name: Analyze Backend Test Results
      if: ${{ (inputs.target_component == 'backend' || inputs.target_component == 'both' || inputs.target_component == null) && steps.backend-test-coverage.outcome != 'skipped' }}
      id: analyze-backend
      run: |
        echo "ğŸ” Analyzing backend test results..."
        cd backend
        
        # Check if tests failed or missing (exit code OR output contains failures)
        test_exit_code="${{ steps.backend-test-coverage.outputs.backend_test_exit_code }}"
        has_test_failures="false"
        
        # Check exit code
        if [ "$test_exit_code" != "0" ]; then
          echo "âŒ Backend tests failed with exit code: $test_exit_code"
          has_test_failures="true"
        fi
        
        # Also check test output for failure indicators
        if [ -f test-output.log ]; then
          if grep -qE "(failed|error:|FAIL|AssertionError|No backend tests)" test-output.log; then
            echo "âŒ Backend test output contains failures/errors or missing tests"
            has_test_failures="true"
          fi
        fi
        
        if [ "$has_test_failures" = "true" ]; then
          echo "needs_fixes=true" >> $GITHUB_OUTPUT
          
          # Save error log for AI analysis
          cp test-output.log ../scripts/backend-test-errors.log
        else
          echo "âœ… Backend tests passed"
          echo "needs_fixes=false" >> $GITHUB_OUTPUT
        fi

    # === AI-POWERED TEST FIXES & GENERATION ===
    - name: AI Fix Frontend Test Errors
      if: ${{ steps.analyze-frontend.outputs.needs_fixes == 'true' }}
      id: fix-frontend-tests
      run: |
        echo "ğŸ¤– AI Frontend Test Error Resolution Pipeline Starting..."
        
        cd scripts
        
        # Phase 1: Enhanced Error Analysis & Context Preparation
        echo "ğŸ“‹ PHASE 1: Enhanced Error Analysis & Context Preparation"
        echo "=============================="
        
        # Gather comprehensive context about the frontend environment
        echo "ğŸ” Gathering frontend environment context..."
        
        # Check frontend package.json for test configuration
        if [ -f ../ui/package.json ]; then
          echo "   ğŸ“¦ Frontend package.json detected"
          echo "   ğŸ”§ Test scripts available:"
          cat ../ui/package.json | jq -r '.scripts | to_entries[] | select(.key | contains("test")) | "      \(.key): \(.value)"' 2>/dev/null || echo "      (Could not parse test scripts)"
          
          echo "   ğŸ“š Testing dependencies:"
          cat ../ui/package.json | jq -r '.devDependencies | to_entries[] | select(.key | test("test|vitest|jest|cypress|playwright")) | "      \(.key): \(.value)"' 2>/dev/null || echo "      (Could not parse dependencies)"
        else
          echo "   âš ï¸ No package.json found in ui/ directory"
        fi
        
        # Check if vitest config exists
        echo "   âš™ï¸ Test configuration files:"
        for config_file in ../ui/vitest.config.ts ../ui/vitest.config.js ../ui/vite.config.ts ../ui/jest.config.js; do
          if [ -f "$config_file" ]; then
            echo "      âœ… Found: $(basename $config_file)"
          fi
        done
        
        # Check existing test structure
        echo "   ğŸ“ Existing test structure:"
        if [ -d ../ui/test ]; then
          test_count=$(find ../ui/test -name "*.test.*" -o -name "*.spec.*" | wc -l)
          echo "      ğŸ“Š Test files found: $test_count"
          if [ $test_count -gt 0 ]; then
            echo "      ğŸ“‚ Test directories:"
            find ../ui/test -type d | head -5 | sed 's|../ui/test|      ui/test|'
            echo "      ğŸ“„ Sample test files:"
            find ../ui/test -name "*.test.*" -o -name "*.spec.*" | head -3 | sed 's|../ui/|      ui/|'
          fi
        else
          echo "      âš ï¸ No test directory found"
        fi
        
        # Check source code structure to understand what needs testing
        echo "   ğŸ“‚ Source code structure (for test generation context):"
        if [ -d ../ui/src ]; then
          component_count=$(find ../ui/src -name "*.tsx" -o -name "*.jsx" | wc -l)
          echo "      ğŸ§© React components found: $component_count"
          if [ $component_count -gt 0 ]; then
            echo "      ğŸ“„ Sample components:"
            find ../ui/src -name "*.tsx" -o -name "*.jsx" | head -3 | sed 's|../ui/|      ui/|'
          fi
          
          util_count=$(find ../ui/src -name "*.ts" -not -name "*.test.*" -not -name "*.spec.*" | wc -l)
          echo "      ğŸ”§ TypeScript utilities/modules: $util_count"
        fi
        
        # Enhance error log with comprehensive context for Junior AI
        echo ""
        echo "ğŸ“ Creating enhanced error analysis log for AI processing..."
        echo "=== FRONTEND TEST ERROR ANALYSIS ===" > enhanced-frontend-errors.log
        echo "Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> enhanced-frontend-errors.log
        echo "Component: Frontend (UI)" >> enhanced-frontend-errors.log
        echo "Task: Fix failing tests and resolve test configuration issues" >> enhanced-frontend-errors.log
        echo "Context: Running 'bun run vitest run --coverage' in ui/ directory" >> enhanced-frontend-errors.log
        echo "Target Coverage: ${{ inputs.coverage_threshold || 80 }}%" >> enhanced-frontend-errors.log
        echo "" >> enhanced-frontend-errors.log
        
        # Add environment context to the log
        echo "=== ENVIRONMENT CONTEXT ===" >> enhanced-frontend-errors.log
        echo "Node.js Runtime: Bun" >> enhanced-frontend-errors.log
        echo "Test Framework: Vitest (expected)" >> enhanced-frontend-errors.log
        echo "Component Framework: React + TypeScript" >> enhanced-frontend-errors.log
        echo "Build Tool: Vite" >> enhanced-frontend-errors.log
        echo "" >> enhanced-frontend-errors.log
        
        # Add test structure context
        if [ -d ../ui/test ]; then
          echo "=== EXISTING TEST STRUCTURE ===" >> enhanced-frontend-errors.log
          find ../ui/test -type f | head -10 >> enhanced-frontend-errors.log
          echo "" >> enhanced-frontend-errors.log
        fi
        
        # Add the actual error log
        echo "=== ORIGINAL TEST ERROR OUTPUT ===" >> enhanced-frontend-errors.log
        cat frontend-test-errors.log >> enhanced-frontend-errors.log
        
        log_lines=$(wc -l < enhanced-frontend-errors.log)
        echo "   âœ… Enhanced error log created: $log_lines lines of comprehensive context"
        echo "   ğŸ“„ Log includes: environment details, existing structure, and error output"
        
        # Phase 2: Junior AI Proposer - Interactive Code Exploration & Fix Generation
        echo ""
        echo "ğŸ§  PHASE 2: Junior AI Proposer - Interactive Code Exploration & Fix Generation"
        echo "=============================="
        echo "ğŸ¤– Junior AI Capabilities:"
        echo "   â€¢ ğŸ” MCP-based code exploration tools (semantic search, file reading)"
        echo "   â€¢ ğŸ§­ Dynamic workspace navigation and dependency analysis"  
        echo "   â€¢ ğŸ¯ Context-aware pattern recognition and solution generation"
        echo "   â€¢ ğŸ¤ Friend AI collaboration for complex problem-solving"
        echo "   â€¢ ğŸ“Š Real-time error analysis and root cause identification"
        echo "   â€¢ ğŸ”§ Precise fix generation with validation"
        echo ""

        
        start_time=$(date +%s)
        echo "â±ï¸ Junior AI Proposer started at: $(date '+%H:%M:%S')"
        
        python test-enhanced-proposer.py enhanced-frontend-errors.log > frontend-proposed-fixes.json 2> frontend-proposer.log
        proposer_exit_code=$?
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        echo ""
        echo "â±ï¸ Junior AI Proposer completed in: ${duration}s"
        echo "ğŸ“Š Exit Code: $proposer_exit_code"
        
        if [ $proposer_exit_code -eq 0 ]; then
          if [ -f frontend-proposed-fixes.json ]; then
            # Validate JSON structure
            if jq . frontend-proposed-fixes.json >/dev/null 2>&1; then
              proposed_count=$(jq -r '.changes | length' frontend-proposed-fixes.json 2>/dev/null || echo "0")
              analysis_length=$(jq -r '.analysis | length' frontend-proposed-fixes.json 2>/dev/null || echo "0")
              
              echo "âœ… Junior AI Proposer: SUCCESSFUL COMPLETION!"
              echo "   ğŸ“Š Analysis Quality:"
              echo "      â€¢ Proposed changes: $proposed_count"
              echo "      â€¢ Analysis length: $analysis_length characters"
              echo "      â€¢ JSON structure: Valid âœ…"
              echo "   ğŸ“„ Detailed logs: frontend-proposer.log ($(wc -l < frontend-proposer.log 2>/dev/null || echo "0") lines)"
              
              if [ "$proposed_count" -gt "0" ]; then
                echo "   ğŸ¯ Proposed Changes Preview:"
                jq -r '.changes[] | "      \(.action | ascii_upcase): \(.file)"' frontend-proposed-fixes.json 2>/dev/null | head -8
                if [ "$proposed_count" -gt "8" ]; then
                  echo "      ... and $((proposed_count - 8)) more changes"
                fi
                
                echo "   ğŸ”¬ Change Types Distribution:"
                jq -r '.changes | group_by(.action) | map("\(.length) \(.[0].action)") | join(", ")' frontend-proposed-fixes.json 2>/dev/null | sed 's/^/      /'
              else
                echo "   âš ï¸ No changes proposed - this may indicate the AI needs more context"
              fi
            else
              echo "âŒ Junior AI Proposer: JSON PARSING ERROR!"
              echo "   ğŸ“„ Raw output preview (first 200 chars):"
              head -c 200 frontend-proposed-fixes.json 2>/dev/null | sed 's/^/      /' || echo "      (Could not read file)"
            fi
          else
            echo "âŒ Junior AI Proposer: OUTPUT FILE MISSING!"
            echo "   ğŸ“„ Expected: frontend-proposed-fixes.json"
            echo "   ğŸ“‹ Working directory contents:"
            ls -la | grep -E "(frontend|json)" | sed 's/^/      /' || echo "      (No relevant files found)"
          fi
        else
          echo "âŒ Junior AI Proposer: EXECUTION FAILED!"
          echo "   ğŸ’¥ Exit code: $proposer_exit_code"
          echo "   ğŸ“„ Error details from frontend-proposer.log:"
          if [ -f frontend-proposer.log ]; then
            echo "      Log size: $(wc -l < frontend-proposer.log) lines"
            echo "      Last 5 lines:"
            tail -5 frontend-proposer.log 2>/dev/null | sed 's/^/         >' || echo "         >(Could not read log)"
          else
            echo "      âŒ No error log file found"
          fi
        fi
        
        # Phase 3: Senior AI Reviewer - Critical Review & Validation (Optional)
        echo ""
        echo "ğŸ“ PHASE 3: Senior AI Reviewer - Critical Review & Validation"
        echo "=============================="
        
        # Comprehensive validation of Junior AI output before Senior AI review
        echo "ğŸ” Pre-Review Validation of Junior AI Output..."
        junior_analysis=""
        junior_changes_count="0"
        skip_senior_review=false
        
        if [ -f frontend-proposed-fixes.json ]; then
          # Extract analysis and changes for validation
          junior_analysis=$(jq -r '.analysis // ""' frontend-proposed-fixes.json 2>/dev/null || echo "")
          junior_changes_count=$(jq -r '.changes | length' frontend-proposed-fixes.json 2>/dev/null || echo "0")
          
          echo "   ğŸ“Š Junior AI Output Metrics:"
          echo "      â€¢ Analysis length: ${#junior_analysis} characters"
          echo "      â€¢ Proposed changes: $junior_changes_count"
          echo "      â€¢ JSON validity: $(jq . frontend-proposed-fixes.json >/dev/null 2>&1 && echo "Valid âœ…" || echo "Invalid âŒ")"
          
          # Check for critical error indicators in analysis
          error_indicators=()
          if echo "$junior_analysis" | grep -qiE "error occurred"; then
            error_indicators+=("error occurred")
          fi
          if echo "$junior_analysis" | grep -qiE "timeout|timed out"; then
            error_indicators+=("timeout")
          fi
          if echo "$junior_analysis" | grep -qiE "failed|failure"; then
            error_indicators+=("failure")
          fi
          if echo "$junior_analysis" | grep -qiE "max iterations|maximum.*reached"; then
            error_indicators+=("max iterations")
          fi
          if echo "$junior_analysis" | grep -qiE "api call failed|api.*error"; then
            error_indicators+=("API error")
          fi
          if echo "$junior_analysis" | grep -qiE "giving up|unable to.*proceed"; then
            error_indicators+=("giving up")
          fi
          
          if [ ${#error_indicators[@]} -gt 0 ]; then
            echo "   âš ï¸ Error Indicators Found: ${error_indicators[*]}"
            echo "   ğŸ“„ Analysis excerpt (first 150 chars): $(echo "$junior_analysis" | head -c 150)..."
            
            if [ "$junior_changes_count" = "0" ] || [ "$junior_changes_count" = "null" ]; then
              echo "   âŒ DECISION: Skip Senior AI review (no usable changes)"
              echo "      Reason: Junior AI failed with critical errors and produced no changes"
              skip_senior_review=true
            else
              echo "   âœ… DECISION: Proceed with Senior AI review"
              echo "      Reason: Despite errors, Junior AI provided $junior_changes_count usable changes"
            fi
          else
            echo "   âœ… No critical error indicators found in Junior AI analysis"
            echo "   âœ… DECISION: Proceed with Senior AI review"
          fi
        else
          echo "   âŒ DECISION: Skip Senior AI review (no output file)"
          skip_senior_review=true
        fi
        
        # Perform Senior AI Review if validation passed
        if [ "$skip_senior_review" = "false" ]; then
          echo ""
          echo "ğŸ¤– Senior AI Review Capabilities:"
          echo "   â€¢ ğŸ” Deep code analysis and pattern validation"
          echo "   â€¢ ğŸ§ª Test framework best practices enforcement"
          echo "   â€¢ ğŸ”§ Search pattern precision verification"
          echo "   â€¢ ğŸš€ Performance and maintainability optimization"
          echo "   â€¢ ğŸ›¡ï¸ Security and error handling improvements"
          echo "   â€¢ ğŸ“š Documentation and code clarity enhancements"
          echo ""
          echo "ğŸ“‹ Review Checklist:"
          echo "   âœ“ Validate search patterns against actual file content"
          echo "   âœ“ Check syntax correctness and TypeScript compliance"
          echo "   âœ“ Verify test framework best practices (Vitest/Jest)"
          echo "   âœ“ Ensure changes address root cause of errors"
          echo "   âœ“ Optimize for maintainability and performance"
          echo "   âœ“ Refine or reject potentially problematic changes"
          echo ""
          
          review_start_time=$(date +%s)
          echo "â±ï¸ Senior AI Reviewer started at: $(date '+%H:%M:%S')"
          
          python review-changes.py frontend-proposed-fixes.json enhanced-frontend-errors.log > frontend-reviewed-fixes.json 2> frontend-reviewer.log
          reviewer_exit_code=$?
          review_end_time=$(date +%s)
          review_duration=$((review_end_time - review_start_time))
          
          echo ""
          echo "â±ï¸ Senior AI Reviewer completed in: ${review_duration}s"
          echo "ğŸ“Š Exit Code: $reviewer_exit_code"
        else
          echo ""
          echo "ğŸš« SKIPPING Senior AI Review"
          echo "   Reason: Junior AI output validation failed"
          reviewer_exit_code=1  # Set to failed to use original proposals
        fi
        
        # Evaluate review results and determine final file to use
        final_fixes_file="frontend-proposed-fixes.json"
        
        if [ $reviewer_exit_code -eq 0 ]; then
          if [ -f frontend-reviewed-fixes.json ]; then
            # Validate reviewed output
            if jq . frontend-reviewed-fixes.json >/dev/null 2>&1; then
              reviewed_count=$(jq -r '.changes | length' frontend-reviewed-fixes.json 2>/dev/null || echo "0")
              
              if [ "$reviewed_count" -gt "0" ]; then
                echo "âœ… Senior AI Reviewer: SUCCESSFUL COMPLETION!"
                echo "   ğŸ“Š Review Results:"
                echo "      â€¢ Original changes: $junior_changes_count"
                echo "      â€¢ Reviewed changes: $reviewed_count"
                echo "      â€¢ Change delta: $((reviewed_count - junior_changes_count))"
                echo "      â€¢ JSON structure: Valid âœ…"
                echo "   ğŸ“„ Review log: frontend-reviewer.log ($(wc -l < frontend-reviewer.log 2>/dev/null || echo "0") lines)"
                
                # Show review summary if available
                review_summary=$(jq -r '.analysis // ""' frontend-reviewed-fixes.json 2>/dev/null || echo "")
                if [ ${#review_summary} -gt 50 ]; then
                  echo "   ğŸ¯ Review Summary (first 200 chars):"
                  echo "      $(echo "$review_summary" | head -c 200 | tr '\n' ' ')..."
                fi
                
                final_fixes_file="frontend-reviewed-fixes.json"
                
                if [ "$junior_changes_count" != "$reviewed_count" ]; then
                  echo "   ğŸ“ Senior AI Modifications:"
                  if [ "$reviewed_count" -lt "$junior_changes_count" ]; then
                    echo "      ğŸ”„ Reduced changes by $((junior_changes_count - reviewed_count)) (optimization/rejection)"
                  else
                    echo "      ğŸ”„ Added $((reviewed_count - junior_changes_count)) changes (enhancement)"
                  fi
                fi
              else
                echo "âš ï¸ Senior AI Reviewer: EMPTY OUTPUT!"
                echo "   ğŸ“Š Reviewed changes: 0 (using original proposals instead)"
                echo "   ğŸ“„ This may indicate overly strict review criteria"
              fi
            else
              echo "âŒ Senior AI Reviewer: JSON PARSING ERROR!"
              echo "   ğŸ“„ Using original proposals due to invalid reviewed output"
            fi
          else
            echo "âš ï¸ Senior AI Reviewer: OUTPUT FILE MISSING!"
            echo "   ğŸ“„ Expected: frontend-reviewed-fixes.json"
            echo "   ğŸ“„ Using original proposals instead"
          fi
        else
          echo "âŒ Senior AI Reviewer: EXECUTION FAILED!"
          echo "   ğŸ’¥ Exit code: $reviewer_exit_code"
          echo "   ğŸ“„ Error details from frontend-reviewer.log:"
          if [ -f frontend-reviewer.log ]; then
            echo "      Log size: $(wc -l < frontend-reviewer.log) lines"
            echo "      Last 3 lines:"
            tail -3 frontend-reviewer.log 2>/dev/null | sed 's/^/         >' || echo "         >(Could not read log)"
          fi
          echo "   ğŸ”„ Continuing with original Junior AI proposals..."
        fi
        
        echo ""
        echo "ğŸ“‹ Final Decision: Will apply changes from '$final_fixes_file'"
        if [ -f "$final_fixes_file" ]; then
          final_count=$(jq -r '.changes | length' "$final_fixes_file" 2>/dev/null || echo "0")
          echo "   ğŸ“Š Final change count: $final_count"
        fi
        
        # Phase 4: Automated Applier - Precise File Modifications
        echo ""
        echo "ğŸ”§ PHASE 4: Automated Applier - Precise File Modifications"
        echo "=============================="
        echo "âš™ï¸ Applier Capabilities:"
        echo "   â€¢ ğŸ¯ Precise string-based search and replace operations"
        echo "   â€¢ ğŸ“ Intelligent file creation with directory structure"
        echo "   â€¢ ğŸ” Pre-application validation of search patterns"
        echo "   â€¢ ğŸ›¡ï¸ Backup and rollback mechanisms for safety"
        echo "   â€¢ ğŸ“ Detailed operation logging and error reporting"
        echo "   â€¢ ğŸ”„ Git integration with descriptive commit messages"
        echo ""
        
        apply_start_time=$(date +%s)
        echo "â±ï¸ Automated Applier started at: $(date '+%H:%M:%S')"
        
        # Pre-application validation
        echo "ğŸ” Pre-Application Validation:"
        if [ -f "$final_fixes_file" ]; then
          echo "   âœ… Final fixes file exists: $final_fixes_file"
          if jq . "$final_fixes_file" >/dev/null 2>&1; then
            echo "   âœ… JSON structure is valid"
            validation_count=$(jq -r '.changes | length' "$final_fixes_file" 2>/dev/null || echo "0")
            echo "   ğŸ“Š Changes to apply: $validation_count"
            
            if [ "$validation_count" -gt "0" ]; then
              echo "   ğŸ¯ Change Distribution:"
              jq -r '.changes | group_by(.action) | map("      \(.length)x \(.[0].action | ascii_upcase)") | join("\n")' "$final_fixes_file" 2>/dev/null || echo "      (Could not analyze distribution)"
            fi
          else
            echo "   âŒ JSON structure is invalid - applier may fail"
          fi
        else
          echo "   âŒ Final fixes file missing: $final_fixes_file"
        fi
        
        python apply-changes.py "$final_fixes_file" > frontend-apply-result.json 2> frontend-applier.log
        applier_exit_code=$?
        apply_end_time=$(date +%s)
        apply_duration=$((apply_end_time - apply_start_time))
        
        echo ""
        echo "â±ï¸ Automated Applier completed in: ${apply_duration}s"
        echo "ğŸ“Š Exit Code: $applier_exit_code"
        
        if [ $applier_exit_code -eq 0 ]; then
          if [ -f frontend-apply-result.json ]; then
            # Parse application results with comprehensive validation
            if jq . frontend-apply-result.json >/dev/null 2>&1; then
              applied=$(jq -r '.changes_applied // 0' frontend-apply-result.json 2>/dev/null || echo "0")
              total=$(jq -r '.total_changes // 0' frontend-apply-result.json 2>/dev/null || echo "0")
              errors=$(jq -r '.errors | length // 0' frontend-apply-result.json 2>/dev/null || echo "0")
              
              echo "âœ… Automated Applier: SUCCESSFUL COMPLETION!"
              echo "   ğŸ“Š Application Results:"
              echo "      â€¢ Changes applied: $applied"
              echo "      â€¢ Total changes: $total"
              echo "      â€¢ Success rate: $([ "$total" -gt "0" ] && echo "scale=1; $applied * 100 / $total" | bc -l || echo "0")%"
              echo "      â€¢ Errors encountered: $errors"
              echo "   ğŸ“„ Detailed log: frontend-applier.log ($(wc -l < frontend-applier.log 2>/dev/null || echo "0") lines)"
              
              if [ "$errors" -gt "0" ]; then
                echo "   âš ï¸ Application Errors ($errors total):"
                jq -r '.errors[]?' frontend-apply-result.json 2>/dev/null | head -5 | sed 's/^/      â€¢ /' || echo "      (Could not parse errors)"
                if [ "$errors" -gt "5" ]; then
                  echo "      ... and $((errors - 5)) more errors (see frontend-applier.log)"
                fi
              fi
              
              # Show successful operations if any
              if [ "$applied" -gt "0" ]; then
                echo "   âœ… Successfully Applied Changes:"
                jq -r '.applied_changes[]?' frontend-apply-result.json 2>/dev/null | head -5 | sed 's/^/      â€¢ /' || echo "      (Details in apply result log)"
              fi
            else
              echo "âŒ Automated Applier: JSON PARSING ERROR in result!"
              echo "   ğŸ“„ Raw result preview (first 200 chars):"
              head -c 200 frontend-apply-result.json 2>/dev/null | sed 's/^/      /' || echo "      (Could not read result file)"
            fi
          else
            echo "âš ï¸ Automated Applier: OUTPUT FILE MISSING!"
            echo "   ï¿½ Expected: frontend-apply-result.json"
            echo "   ğŸ“‹ Working directory contents:"
            ls -la | grep -E "(apply|result)" | sed 's/^/      /' || echo "      (No result files found)"
          fi
        else
          echo "âŒ Automated Applier: EXECUTION FAILED!"
          echo "   ğŸ’¥ Exit code: $applier_exit_code"
          echo "   ğŸ“„ Error details from frontend-applier.log:"
          if [ -f frontend-applier.log ]; then
            echo "      Log size: $(wc -l < frontend-applier.log) lines"
            echo "      Critical errors (last 5 lines):"
            tail -5 frontend-applier.log 2>/dev/null | sed 's/^/         >' || echo "         >(Could not read applier log)"
          else
            echo "      âŒ No applier log file found"
          fi
        fi
        
        # Phase 5: Git Operations & Result Verification  
        echo ""
        echo "ğŸ”„ PHASE 5: Git Operations & Result Verification"
        echo "=============================="
        echo "ğŸŒ Git Integration Analysis:"
        
        # Extract application metrics for git analysis
        if [ -f frontend-apply-result.json ] && jq . frontend-apply-result.json >/dev/null 2>&1; then
          applied=$(jq -r '.changes_applied // 0' frontend-apply-result.json 2>/dev/null || echo "0")
          total=$(jq -r '.total_changes // 0' frontend-apply-result.json 2>/dev/null || echo "0")
        else
          applied="0"
          total="0"
        fi
        
        echo "   ğŸ“Š Changes Applied: $applied (out of $total proposed)"
        
        if [ "$applied" -gt "0" ]; then
          echo ""
          echo "ğŸ“‹ Repository Status Analysis:"
          
          # Current branch info
          current_branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo "unknown")
          echo "   ğŸŒ¿ Current branch: $current_branch"
          
          # Check working directory status
          echo "   ğŸ“ Working directory status:"
          if git status --porcelain | grep -q .; then
            modified_files=$(git status --porcelain | wc -l)
            echo "      â€¢ Modified files: $modified_files"
            echo "      â€¢ File details:"
            git status --porcelain | head -5 | sed 's/^/         /' || echo "         (Could not read git status)"
            if [ "$modified_files" -gt "5" ]; then
              echo "         ... and $((modified_files - 5)) more files"
            fi
          else
            echo "      âœ… Working directory clean"
          fi
          
          # Recent commit analysis
          echo ""
          echo "   ğŸ“ Recent Commit History:"
          echo "      Last 3 commits:"
          git log --oneline -3 2>/dev/null | sed 's/^/         /' || echo "         (Could not read git log)"
          
          # Remote sync status
          echo ""
          echo "   ğŸŒ Remote Synchronization Analysis:"
          if git remote -v | grep -q origin; then
            echo "      ğŸ”— Remote 'origin' configured: âœ…"
            
            # Check if we can reach the remote
            if git ls-remote --heads origin >/dev/null 2>&1; then
              echo "      ğŸŒ Remote accessibility: âœ…"
              
              # Compare local vs remote
              local_commits=$(git rev-list --count HEAD ^origin/$current_branch 2>/dev/null || echo "unknown")
              remote_commits=$(git rev-list --count origin/$current_branch ^HEAD 2>/dev/null || echo "unknown")
              
              echo "      ğŸ“Š Sync Status:"
              echo "         â€¢ Local commits ahead: $local_commits"
              echo "         â€¢ Remote commits ahead: $remote_commits"
              
              if [ "$local_commits" = "0" ]; then
                echo "      âœ… RESULT: Local and remote are in sync - changes were pushed successfully!"
              elif [ "$local_commits" != "unknown" ] && [ "$local_commits" -gt "0" ]; then
                echo "      âš ï¸ RESULT: Local commits NOT pushed to remote!"
                echo "         ğŸ“¤ This indicates a git push failure in apply-changes.py"
                echo "         ğŸ”§ Potential causes:"
                echo "            â€¢ Network connectivity issues"
                echo "            â€¢ Authentication/permission problems"
                echo "            â€¢ Branch protection rules"
                echo "            â€¢ Merge conflicts with remote"
              else
                echo "      â“ RESULT: Unable to determine sync status"
              fi
            else
              echo "      âŒ Remote accessibility: Failed (network/auth issue)"
            fi
          else
            echo "      âš ï¸ No remote 'origin' configured"
          fi
          
          # Check for any git errors in the applier log
          echo ""
          echo "   ğŸ” Git Operation Log Analysis:"
          if [ -f frontend-applier.log ]; then
            git_errors=$(grep -i "git.*error\|push.*failed\|permission denied\|authentication" frontend-applier.log | wc -l)
            if [ "$git_errors" -gt "0" ]; then
              echo "      âš ï¸ Git errors found in applier log ($git_errors lines):"
              grep -i "git.*error\|push.*failed\|permission denied\|authentication" frontend-applier.log | head -3 | sed 's/^/         >' || true
            else
              echo "      âœ… No git errors detected in applier log"
            fi
          else
            echo "      â“ No applier log available for analysis"
          fi
        else
          echo "   â„¹ï¸ No changes were applied, so no git operations were expected"
        fi
        
        # Set output for next steps
        echo "frontend_fixes_applied=$applied" >> $GITHUB_OUTPUT
        
        echo ""
        echo "ğŸ‰ FRONTEND TEST ERROR RESOLUTION PIPELINE COMPLETE!"
        echo "=============================="
        echo "ğŸ“Š Final Summary:"
        echo "   â€¢ Total runtime: $(($(date +%s) - start_time))s"
        echo "   â€¢ Changes successfully applied: $applied"
        echo "   â€¢ Junior AI proposer: $([ $proposer_exit_code -eq 0 ] && echo "âœ… Success" || echo "âŒ Failed ($proposer_exit_code)")"
        echo "   â€¢ Senior AI reviewer: $([ $reviewer_exit_code -eq 0 ] && echo "âœ… Success" || echo "âŒ Failed/Skipped ($reviewer_exit_code)")"
        echo "   â€¢ Automated applier: $([ $applier_exit_code -eq 0 ] && echo "âœ… Success" || echo "âŒ Failed ($applier_exit_code)")"
        echo "   â€¢ Git operations: $([ "$applied" -gt "0" ] && echo "ğŸ”„ Changes committed" || echo "â„¹ï¸ No changes to commit")"
        echo ""
        echo "ğŸ“ Generated Artifacts:"
        echo "   â€¢ enhanced-frontend-errors.log (input context)"
        echo "   â€¢ frontend-proposed-fixes.json (Junior AI output)"
        echo "   â€¢ frontend-reviewed-fixes.json (Senior AI output)"
        echo "   â€¢ frontend-apply-result.json (application results)"
        echo "   â€¢ frontend-proposer.log, frontend-reviewer.log, frontend-applier.log"
        echo ""

    - name: AI Fix Backend Test Errors
      if: ${{ steps.analyze-backend.outputs.needs_fixes == 'true' }}
      id: fix-backend-tests
      run: |
        echo "ğŸ¤– Junior AI fixing backend test errors..."
        cd scripts
        
        # Enhance error log with context for Junior AI
        echo "=== BACKEND TEST ERROR ANALYSIS ===" > enhanced-backend-errors.log
        echo "Component: Backend (Node.js/TypeScript)" >> enhanced-backend-errors.log
        echo "Task: Setup test framework and create initial test suite if missing, or fix failing tests" >> enhanced-backend-errors.log
        echo "Context: Running 'bun run test --coverage' in backend/ directory" >> enhanced-backend-errors.log
        echo "" >> enhanced-backend-errors.log
        cat backend-test-errors.log >> enhanced-backend-errors.log
        
        echo "ğŸ“ Enhanced backend error log created with $(wc -l < enhanced-backend-errors.log) lines of context"
        
        # Phase 1: Junior AI Proposer - Backend Test Architecture & Fixes
        echo ""
        echo "ğŸ§  PHASE 1: Junior AI Proposer Starting Backend Test Analysis..."
        echo "=============================="
        echo "ğŸ¯ Junior AI will:"
        echo "   â€¢ Analyze backend test framework requirements"
        echo "   â€¢ Explore existing API routes and business logic"
        echo "   â€¢ Design appropriate test architecture for Node.js/TypeScript"
        echo "   â€¢ Create test setup files and configuration if missing"
        echo "   â€¢ Generate API endpoint tests and unit tests"
        
        python test-enhanced-proposer.py enhanced-backend-errors.log > backend-proposed-fixes.json 2> backend-proposer.log
        proposer_exit_code=$?
        
        echo ""
        if [ $proposer_exit_code -eq 0 ]; then
          if [ -f backend-proposed-fixes.json ]; then
            proposed_count=$(cat backend-proposed-fixes.json | jq -r '.changes | length' 2>/dev/null || echo "0")
            echo "âœ… Junior AI Proposer completed backend analysis!"
            echo "   ğŸ“Š Proposed changes: $proposed_count"
            echo "   ğŸ“„ Detailed analysis saved to backend-proposer.log"
            
            if [ "$proposed_count" -gt "0" ]; then
              echo "   ğŸ¯ Backend change preview:"
              cat backend-proposed-fixes.json | jq -r '.changes[] | "      â€¢ \(.action) \(.file): \(.reasoning)"' 2>/dev/null | head -5
            fi
          fi
        else
          echo "âŒ Junior AI Proposer failed for backend with exit code: $proposer_exit_code"
          tail -10 backend-proposer.log 2>/dev/null || echo "   No error log available"
        fi
        
        # Phase 2: Senior AI Reviewer - Backend-Specific Validation (Optional)
        echo ""
        echo "ğŸ“ PHASE 2: Senior AI Reviewer Validating Backend Changes (Optional)..."
        echo "=============================="
        
        # First validate Junior AI output before sending to Senior AI
        echo "ğŸ” Validating Junior AI output quality..."
        junior_analysis=$(cat backend-proposed-fixes.json | jq -r '.analysis // ""' 2>/dev/null || echo "")
        junior_changes_count=$(cat backend-proposed-fixes.json | jq -r '.changes | length' 2>/dev/null || echo "0")
        
        # Check for error indicators in Junior AI analysis
        skip_senior_review=false
        if echo "$junior_analysis" | grep -iE "(error occurred|timeout|failed|max iterations|api call failed|all attempts|giving up)" > /dev/null; then
          echo "âš ï¸ Junior AI encountered issues, checking if output is still usable..."
          echo "   Analysis excerpt: $(echo "$junior_analysis" | head -c 100)..."
          
          if [ "$junior_changes_count" = "0" ] || [ "$junior_changes_count" = "null" ]; then
            echo "âŒ Junior AI failed with no usable changes, skipping Senior AI review"
            skip_senior_review=true
          else
            echo "âœ… Junior AI provided $junior_changes_count changes despite issues, proceeding with review"
          fi
        else
          echo "âœ… Junior AI output looks good, proceeding with Senior AI review"
        fi
        
        if [ "$skip_senior_review" = "false" ]; then
          echo "ğŸ” Senior AI will validate:"
          echo "   â€¢ API testing patterns and best practices"
          echo "   â€¢ Test database setup and teardown procedures"
          echo "   â€¢ Mocking strategies for external dependencies"
          echo "   â€¢ Integration test coverage for critical paths"
          echo "   â€¢ Performance test considerations"
          echo "   â€¢ If review fails, we'll continue with original proposals"
          
          python review-changes.py backend-proposed-fixes.json enhanced-backend-errors.log > backend-reviewed-fixes.json 2> backend-reviewer.log
          reviewer_exit_code=$?
        else
          echo "ğŸš« Skipping Senior AI review due to Junior AI failure"
          reviewer_exit_code=1  # Set to failed to use original proposals
        fi
        
        # Set the file to use for application (reviewed if successful, original if failed)
        backend_final_fixes_file="backend-proposed-fixes.json"
        
        echo ""
        if [ $reviewer_exit_code -eq 0 ]; then
          if [ -f backend-reviewed-fixes.json ]; then
            reviewed_count=$(cat backend-reviewed-fixes.json | jq -r '.changes | length' 2>/dev/null || echo "0")
            if [ "$reviewed_count" -gt "0" ]; then
              echo "âœ… Senior AI Reviewer completed backend validation!"
              echo "   ğŸ“Š Reviewed changes: $reviewed_count"
              echo "   ğŸ“„ Detailed review saved to backend-reviewer.log"
              backend_final_fixes_file="backend-reviewed-fixes.json"
            else
              echo "âš ï¸ Senior AI review produced empty changes, using original backend proposals"
            fi
          else
            echo "âš ï¸ Senior AI completed but no reviewed fixes file generated, using original backend proposals"
          fi
        else
          echo "âŒ Senior AI Reviewer failed for backend with exit code: $reviewer_exit_code"
          echo "ğŸ“„ Error details:"
          tail -10 backend-reviewer.log 2>/dev/null || echo "   No error log available"
          echo "ğŸ”„ Continuing with original Junior AI backend proposals..."
        fi
        
        echo "ğŸ“‹ Will apply backend changes from: $backend_final_fixes_file"
        
        # Phase 3: Automated Applier - Backend Implementation
        echo ""
        echo "ğŸ”§ PHASE 3: Applying Backend Test Infrastructure..."
        echo "=============================="
        
        python apply-changes.py "$backend_final_fixes_file" > backend-apply-result.json 2> backend-applier.log
        applier_exit_code=$?
        
        echo ""
        if [ $applier_exit_code -eq 0 ]; then
          if [ -f backend-apply-result.json ]; then
            applied=$(cat backend-apply-result.json | jq -r '.changes_applied // 0')
            total=$(cat backend-apply-result.json | jq -r '.total_changes // 0')
            
            echo "âœ… Backend test infrastructure applied successfully!"
            echo "   ğŸ“Š Changes applied: $applied/$total"
            echo "   ğŸ“„ Detailed application log saved to backend-applier.log"
            
            # ADD DETAILED GIT PUSH LOGGING FOR BACKEND
            echo ""
            echo "ğŸ”„ BACKEND GIT COMMIT AND PUSH STATUS:"
            echo "=============================="
            
            if [ "$applied" -gt "0" ]; then
              echo "ğŸ“‹ Checking git status after backend changes..."
              git status --porcelain
              
              echo "ğŸ“ Checking recent commits..."
              git log --oneline -3
              
              echo "ğŸŒ Checking if backend changes were pushed to remote..."
              current_branch=$(git rev-parse --abbrev-ref HEAD)
              local_commits=$(git rev-list --count HEAD ^origin/$current_branch 2>/dev/null || echo "0")
              echo "   Local commits ahead of remote: $local_commits"
              
              if [ "$local_commits" -gt "0" ]; then
                echo "   âš ï¸ WARNING: Backend changes were NOT pushed to remote!"
              else
                echo "   âœ… Backend changes were pushed successfully"
              fi
            fi
            
            echo "backend_fixes_applied=$applied" >> $GITHUB_OUTPUT
            echo ""
            echo "ğŸ‰ BACKEND TEST SETUP COMPLETE: $applied changes successfully applied!"
          fi
        else
          echo "âŒ Backend applier failed with exit code: $applier_exit_code"
          tail -10 backend-applier.log 2>/dev/null || echo "   No error log available"
        fi

    - name: AI Generate Missing Frontend Tests
      if: ${{ (inputs.add_missing_tests == true || inputs.add_missing_tests == null) && steps.analyze-frontend.outputs.needs_more_tests == 'true' }}
      id: generate-frontend-tests
      run: |
        echo "ğŸ¤– Junior AI generating missing frontend tests based on coverage..."
        cd scripts
        
        # Create comprehensive coverage analysis for Junior AI
        echo "=== FRONTEND TEST COVERAGE ENHANCEMENT ===" > coverage-analysis.log
        echo "Component: Frontend (React/TypeScript)" >> coverage-analysis.log
        echo "Task: Generate missing tests to improve code coverage" >> coverage-analysis.log
        echo "Current coverage: ${{ steps.analyze-frontend.outputs.coverage_percentage }}%" >> coverage-analysis.log
        echo "Target coverage: ${{ inputs.coverage_threshold || 80 }}%" >> coverage-analysis.log
        echo "Framework: Vitest + Testing Library + Jest DOM" >> coverage-analysis.log
        echo "" >> coverage-analysis.log
        echo "COVERAGE GAPS TO ADDRESS:" >> coverage-analysis.log
        echo "- Create tests for untested components" >> coverage-analysis.log
        echo "- Add edge case scenarios" >> coverage-analysis.log
        echo "- Test error handling paths" >> coverage-analysis.log
        echo "- Add integration tests where needed" >> coverage-analysis.log
        echo "" >> coverage-analysis.log
        
        # Add uncovered files information if available
        if [ -f ../ui/coverage-files.txt ]; then
          echo "UNCOVERED/LOW COVERAGE FILES:" >> coverage-analysis.log
          while IFS= read -r file; do
            if [ -f "$file" ]; then
              echo "File: $file" >> coverage-analysis.log
              # Extract uncovered lines info if available in JSON format
              if command -v jq >/dev/null 2>&1; then
                jq -r '.uncoveredLines // []' "$file" 2>/dev/null | head -10 >> coverage-analysis.log || true
              fi
            fi
          done < ../ui/coverage-files.txt
        fi
        
        # Add examples of existing test patterns for context
        echo "" >> coverage-analysis.log
        echo "EXISTING TEST PATTERNS (for reference):" >> coverage-analysis.log
        find ../ui/test -name "*.test.tsx" -o -name "*.test.ts" 2>/dev/null | head -3 | while read test_file; do
          echo "Example test file: $test_file" >> coverage-analysis.log
          head -20 "$test_file" 2>/dev/null >> coverage-analysis.log || true
          echo "---" >> coverage-analysis.log
        done
        
        # Use Junior AI to generate comprehensive test coverage improvements
        python test-enhanced-proposer.py coverage-analysis.log > frontend-test-proposals.json
        python review-changes.py frontend-test-proposals.json coverage-analysis.log > frontend-test-reviewed.json
        python apply-changes.py frontend-test-reviewed.json > frontend-test-result.json
        
        # Check results
        if [ -f frontend-test-result.json ]; then
          echo "ğŸ“„ Checking frontend-test-result.json for valid JSON..."
          if jq . frontend-test-result.json >/dev/null 2>&1; then
            added=$(cat frontend-test-result.json | jq -r '.changes_applied // 0' 2>/dev/null || echo "0")
            echo "frontend_tests_added=$added" >> $GITHUB_OUTPUT
            echo "ğŸ“ Generated $added frontend test files/improvements"
          else
            echo "âš ï¸ frontend-test-result.json contains invalid JSON, using fallback"
            echo "frontend_tests_added=0" >> $GITHUB_OUTPUT
            echo "ğŸ“„ Raw content preview:"
            head -3 frontend-test-result.json | sed 's/^/   /'
          fi
        else
          echo "âŒ frontend-test-result.json not found"
          echo "frontend_tests_added=0" >> $GITHUB_OUTPUT
        fi

    - name: Re-run Tests After AI Fixes
      if: ${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied > 0 || steps.fix-backend-tests.outputs.backend_fixes_applied > 0 || steps.generate-frontend-tests.outputs.frontend_tests_added > 0 }}
      id: rerun-tests
      run: |
        echo "ğŸ”„ Re-running tests after AI improvements..."
        
        # Re-run frontend tests if they were fixed
        if [ "${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied || 0 }}" -gt "0" ] || [ "${{ steps.generate-frontend-tests.outputs.frontend_tests_added || 0 }}" -gt "0" ]; then
          echo "ğŸ§ª Re-running frontend tests..."
          cd ui
          bun run test:coverage --reporter=verbose 2>&1 | tee ../scripts/frontend-retest.log
          frontend_retest_exit=$?
          echo "frontend_retest_exit_code=$frontend_retest_exit" >> $GITHUB_OUTPUT
          
          # Check new coverage
          if [ -f coverage/coverage-summary.json ]; then
            new_coverage=$(cat coverage/coverage-summary.json | jq -r '.total.lines.pct // 0')
            echo "frontend_new_coverage=$new_coverage" >> $GITHUB_OUTPUT
            echo "ğŸ“Š New frontend coverage: $new_coverage%"
          fi
          cd ..
        fi
        
        # Re-run backend tests if they were fixed
        if [ "${{ steps.fix-backend-tests.outputs.backend_fixes_applied || 0 }}" -gt "0" ]; then
          echo "ğŸ§ª Re-running backend tests..."
          cd backend
          bun run test --coverage 2>&1 | tee ../scripts/backend-retest.log
          backend_retest_exit=$?
          echo "backend_retest_exit_code=$backend_retest_exit" >> $GITHUB_OUTPUT
          cd ..
        fi

    - name: Generate Test Summary
      id: summary
      run: |
        echo "ğŸ“Š Generating test improvement summary..."
        
        # Calculate totals
        frontend_fixes="${{ steps.fix-frontend-tests.outputs.frontend_fixes_applied || 0 }}"
        backend_fixes="${{ steps.fix-backend-tests.outputs.backend_fixes_applied || 0 }}"
        frontend_tests="${{ steps.generate-frontend-tests.outputs.frontend_tests_added || 0 }}"
        
        total_fixes=$((frontend_fixes + backend_fixes))
        total_tests_added=$frontend_tests
        
        echo "tests_added=$total_tests_added" >> $GITHUB_OUTPUT
        echo "errors_fixed=$total_fixes" >> $GITHUB_OUTPUT
        
        # Coverage improvement
        old_coverage="${{ steps.analyze-frontend.outputs.coverage_percentage || 0 }}"
        new_coverage="${{ steps.rerun-tests.outputs.frontend_new_coverage || steps.analyze-frontend.outputs.coverage_percentage || 0 }}"
        
        if (( $(echo "$new_coverage > $old_coverage" | bc -l) )); then
          echo "coverage_improved=true" >> $GITHUB_OUTPUT
          improvement=$(echo "$new_coverage - $old_coverage" | bc -l)
          echo "ğŸ“ˆ Coverage improved by $improvement% ($old_coverage% â†’ $new_coverage%)"
        else
          echo "coverage_improved=false" >> $GITHUB_OUTPUT
        fi
        
        # Create summary comment
        cat > test-summary.md << EOF
        ## ğŸ¤– AI Test Builder Results
        
        ### ğŸ“Š Summary
        - **Errors Fixed**: $total_fixes
        - **Tests Added**: $total_tests_added
        - **Coverage**: $old_coverage% â†’ $new_coverage%
        
        ### ğŸ¯ Frontend Results
        - Initial Coverage: ${{ steps.analyze-frontend.outputs.coverage_percentage || 'N/A' }}%
        - Fixes Applied: $frontend_fixes
        - Tests Added: $frontend_tests
        - Final Coverage: ${{ steps.rerun-tests.outputs.frontend_new_coverage || 'N/A' }}%
        - Tests Status: ${{ steps.rerun-tests.outputs.frontend_retest_exit_code == '0' && 'âœ… Passing' || 'âŒ Still failing' }}
        
        ### ğŸ”§ Backend Results
        - Fixes Applied: $backend_fixes
        - Tests Status: ${{ steps.rerun-tests.outputs.backend_retest_exit_code == '0' && 'âœ… Passing' || steps.rerun-tests.outputs.backend_retest_exit_code && 'âŒ Still failing' || 'â„¹ï¸ No changes' }}
        
        ### ğŸ“ Files Modified
        Check the commit history for detailed changes made by the AI.
        EOF
        
        echo "ğŸ“‹ Test summary generated"

    - name: Upload Test Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ai-test-builder-reports
        path: |
          scripts/*.log
          scripts/*.json
          ui/coverage/
          backend/coverage/
          test-summary.md
        retention-days: 30

    - name: Comment on PR
      if: ${{ github.event_name == 'push' }}
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          if (fs.existsSync('test-summary.md')) {
            const summary = fs.readFileSync('test-summary.md', 'utf8');
            
            // Try to find an open PR for this branch
            const prs = await github.rest.pulls.list({
              owner: context.repo.owner,
              repo: context.repo.repo,
              head: `${context.repo.owner}:${context.ref.replace('refs/heads/', '')}`,
              state: 'open'
            });
            
            if (prs.data.length > 0) {
              await github.rest.issues.createComment({
                issue_number: prs.data[0].number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            }
          }

    - name: Create Summary
      if: always()
      run: |
        echo "## ğŸ¤– AI Test Builder Completed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f test-summary.md ]; then
          cat test-summary.md >> $GITHUB_STEP_SUMMARY
        else
          echo "No detailed summary available" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ¯ Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- Review the AI-generated tests and fixes" >> $GITHUB_STEP_SUMMARY
        echo "- Merge changes if tests are passing and coverage improved" >> $GITHUB_STEP_SUMMARY
        echo "- Consider adjusting coverage thresholds if needed" >> $GITHUB_STEP_SUMMARY
